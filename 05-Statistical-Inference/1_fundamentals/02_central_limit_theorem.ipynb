{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Central Limit Theorem: The Foundation of Statistical Inference üéØ‚ú®\n",
    "\n",
    "## Introduction: The \"Magic\" of Statistics\n",
    "\n",
    "In the previous notebook, we observed something remarkable: **no matter what the population looked like, the distribution of sample means was approximately normal!**\n",
    "\n",
    "- Population was normal ‚Üí Sample means were normal ‚úì\n",
    "- But even when we had skewed soil data, sample means were still approximately normal! ü§Ø\n",
    "\n",
    "**Why does this happen?**\n",
    "\n",
    "The answer is the **Central Limit Theorem (CLT)** - arguably the most important theorem in all of statistics!\n",
    "\n",
    "### Why This is \"Magical\" üé©‚ú®\n",
    "\n",
    "The CLT tells us that:\n",
    "- **Regardless of the population distribution** (normal, skewed, uniform, bimodal, anything!)\n",
    "- **Sample means will be approximately normal** for large enough n\n",
    "- This allows us to use normal distribution tools for inference\n",
    "\n",
    "### ML Connection ü§ñ\n",
    "\n",
    "The CLT explains why:\n",
    "- **Ensemble methods work** (averaging predictions)\n",
    "- **Bootstrap works** (resampling and averaging)\n",
    "- **Bagging reduces variance** (bootstrap aggregating)\n",
    "- **Why averaging multiple models is powerful**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. ‚úÖ Understand the **Central Limit Theorem statement** ‚≠ê‚≠ê\n",
    "2. ‚úÖ See CLT work for **any population distribution**\n",
    "3. ‚úÖ Understand the **effect of sample size** on convergence\n",
    "4. ‚úÖ Know when CLT applies (n ‚â• 30 rule of thumb)\n",
    "5. ‚úÖ Apply CLT to real agricultural data\n",
    "6. ‚úÖ Connect CLT to **ML ensemble methods** ‚≠ê‚≠ê\n",
    "\n",
    "‚≠ê‚≠ê = Most critical concept\n",
    "\n",
    "---\n",
    "\n",
    "Let's discover the magic! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Setup: Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set style for beautiful plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Setup complete!\")\n",
    "print(\"üéØ Ready to explore the Central Limit Theorem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. The Central Limit Theorem ‚≠ê‚≠ê\n",
    "\n",
    "### Formal Statement:\n",
    "\n",
    "**Central Limit Theorem**: Let $X_1, X_2, ..., X_n$ be independent and identically distributed (i.i.d.) random variables from ANY distribution with:\n",
    "- Mean: Œº\n",
    "- Variance: œÉ¬≤\n",
    "\n",
    "Then, as n ‚Üí ‚àû, the distribution of the sample mean $\\bar{X}$ approaches a normal distribution:\n",
    "\n",
    "$$\n",
    "\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i \\quad \\xrightarrow{d} \\quad N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n",
    "$$\n",
    "\n",
    "Or equivalently, the **standardized** sample mean:\n",
    "\n",
    "$$\n",
    "Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\quad \\xrightarrow{d} \\quad N(0, 1)\n",
    "$$\n",
    "\n",
    "### What This Means in Plain English:\n",
    "\n",
    "1. üìä **Sample means are normally distributed** (for large n)\n",
    "2. üéØ **Centered at the population mean** (Œº)\n",
    "3. üìè **Spread is SE = œÉ/‚àön** (standard error)\n",
    "4. ‚ú® **Works for ANY population distribution!** (the magic part)\n",
    "\n",
    "### Three Key Conditions:\n",
    "\n",
    "1. ‚úÖ **Independent**: Observations don't affect each other\n",
    "2. ‚úÖ **Identically distributed**: All from the same population\n",
    "3. ‚úÖ **Large n**: Usually n ‚â• 30 (but depends on how non-normal the population is)\n",
    "\n",
    "### Why This is Revolutionary:\n",
    "\n",
    "- We can use **normal distribution tables and tools**\n",
    "- We can construct **confidence intervals**\n",
    "- We can perform **hypothesis tests**\n",
    "- All without knowing the population distribution!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåæ Baseline: CLT with normal population\n",
    "# Start with a normal population (wheat yields)\n",
    "\n",
    "# Population parameters\n",
    "pop_mean = 5.2\n",
    "pop_std = 0.8\n",
    "n_sims = 2000\n",
    "sample_size = 30\n",
    "\n",
    "# Generate normal population\n",
    "population = np.random.normal(pop_mean, pop_std, 100000)\n",
    "\n",
    "# Simulate sampling distribution\n",
    "sample_means = []\n",
    "for _ in range(n_sims):\n",
    "    sample = np.random.choice(population, size=sample_size, replace=False)\n",
    "    sample_means.append(sample.mean())\n",
    "\n",
    "sample_means = np.array(sample_means)\n",
    "\n",
    "# Calculate theoretical values\n",
    "theoretical_mean = pop_mean\n",
    "theoretical_se = pop_std / np.sqrt(sample_size)\n",
    "\n",
    "print(\"üéØ CLT with Normal Population (Baseline):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Population: N(Œº={pop_mean}, œÉ={pop_std})\")\n",
    "print(f\"Sample size: n={sample_size}\")\n",
    "print(f\"Number of samples: {n_sims}\")\n",
    "print(f\"\\nTheoretical sampling distribution: N({theoretical_mean:.2f}, {theoretical_se:.3f})\")\n",
    "print(f\"Empirical mean of sample means: {sample_means.mean():.3f}\")\n",
    "print(f\"Empirical SE of sample means: {sample_means.std():.3f}\")\n",
    "print(f\"\\n‚úì Sample means are normal (population was normal)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 1: Normal ‚Üí Normal (baseline)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Population distribution\n",
    "axes[0].hist(population[:5000], bins=50, alpha=0.7, color='steelblue', \n",
    "             edgecolor='black', density=True)\n",
    "x = np.linspace(population.min(), population.max(), 100)\n",
    "axes[0].plot(x, stats.norm.pdf(x, pop_mean, pop_std), 'r-', linewidth=2, \n",
    "             label=f'N({pop_mean}, {pop_std})')\n",
    "axes[0].axvline(pop_mean, color='black', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Wheat Yield (tons/hectare)', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('Population Distribution (Normal)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Sampling distribution\n",
    "axes[1].hist(sample_means, bins=40, alpha=0.7, color='orange', \n",
    "             edgecolor='black', density=True)\n",
    "x = np.linspace(sample_means.min(), sample_means.max(), 100)\n",
    "axes[1].plot(x, stats.norm.pdf(x, theoretical_mean, theoretical_se), 'r-', \n",
    "             linewidth=2, label=f'N({theoretical_mean}, {theoretical_se:.3f})')\n",
    "axes[1].axvline(theoretical_mean, color='black', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Sample Mean (tons/hectare)', fontsize=11)\n",
    "axes[1].set_ylabel('Density', fontsize=11)\n",
    "axes[1].set_title(f'Sampling Distribution (n={sample_size})', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('CLT Baseline: Normal Population ‚Üí Normal Sampling Distribution ‚úì', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° This is expected! Normal population ‚Üí Normal sample means\")\n",
    "print(\"   But the MAGIC happens with non-normal populations...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. CLT with Non-Normal Distributions ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "### The Real Magic: Works for ANY Distribution!\n",
    "\n",
    "Now we'll demonstrate the true power of the CLT by showing it works for:\n",
    "\n",
    "1. **Uniform Distribution** (flat, symmetric)\n",
    "2. **Exponential Distribution** (highly skewed)\n",
    "3. **Bimodal Distribution** (two peaks, very non-normal)\n",
    "\n",
    "Watch as the CLT transforms these wildly different distributions into approximately normal sampling distributions!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üé≤ Test 1: Uniform Distribution\n",
    "# Example: Fields in a region have uniformly distributed yields between 3 and 8 tons/hectare\n",
    "\n",
    "# Create uniform population\n",
    "a, b = 3.0, 8.0  # uniform bounds\n",
    "uniform_pop = np.random.uniform(a, b, 100000)\n",
    "uniform_mean = (a + b) / 2\n",
    "uniform_std = np.sqrt((b - a)**2 / 12)\n",
    "\n",
    "# Simulate sampling distribution\n",
    "sample_size = 30\n",
    "n_sims = 2000\n",
    "uniform_sample_means = []\n",
    "\n",
    "for _ in range(n_sims):\n",
    "    sample = np.random.choice(uniform_pop, size=sample_size, replace=False)\n",
    "    uniform_sample_means.append(sample.mean())\n",
    "\n",
    "uniform_sample_means = np.array(uniform_sample_means)\n",
    "uniform_se = uniform_std / np.sqrt(sample_size)\n",
    "\n",
    "print(\"üé≤ Test 1: Uniform Distribution\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Population: Uniform({a}, {b})\")\n",
    "print(f\"Population mean: Œº = {uniform_mean:.2f}\")\n",
    "print(f\"Population std: œÉ = {uniform_std:.3f}\")\n",
    "print(f\"Sample size: n = {sample_size}\")\n",
    "print(f\"\\nTheoretical SE = {uniform_se:.3f}\")\n",
    "print(f\"Empirical SE = {uniform_sample_means.std():.3f}\")\n",
    "print(f\"\\n‚úì Sample means are approximately normal (population was uniform!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 2: Uniform ‚Üí Normal\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Uniform population\n",
    "axes[0].hist(uniform_pop[:5000], bins=50, alpha=0.7, color='steelblue', \n",
    "             edgecolor='black', density=True)\n",
    "axes[0].axhline(1/(b-a), color='r', linestyle='-', linewidth=2, \n",
    "                label=f'Uniform({a}, {b})')\n",
    "axes[0].axvline(uniform_mean, color='black', linestyle='--', linewidth=2, \n",
    "                label=f'Œº = {uniform_mean:.1f}')\n",
    "axes[0].set_xlabel('Yield (tons/hectare)', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('Population: UNIFORM (flat!) üìè', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0, 0.35)\n",
    "\n",
    "# Right: Normal sampling distribution\n",
    "axes[1].hist(uniform_sample_means, bins=40, alpha=0.7, color='orange', \n",
    "             edgecolor='black', density=True)\n",
    "x = np.linspace(uniform_sample_means.min(), uniform_sample_means.max(), 100)\n",
    "axes[1].plot(x, stats.norm.pdf(x, uniform_mean, uniform_se), 'r-', \n",
    "             linewidth=2, label=f'N({uniform_mean:.1f}, {uniform_se:.3f})')\n",
    "axes[1].axvline(uniform_mean, color='black', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Sample Mean (tons/hectare)', fontsize=11)\n",
    "axes[1].set_ylabel('Density', fontsize=11)\n",
    "axes[1].set_title(f'Sampling Distribution: NORMAL! üîî', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('‚ú® CLT Magic: Uniform ‚Üí Normal! ‚ú®', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nü§Ø AMAZING! Flat uniform distribution ‚Üí Bell-shaped normal distribution!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚è∞ Test 2: Exponential Distribution (highly skewed!)\n",
    "# Example: Time between pest occurrences (exponential waiting times)\n",
    "\n",
    "# Create exponential population\n",
    "rate = 0.5  # events per day\n",
    "exp_pop = np.random.exponential(scale=1/rate, size=100000)\n",
    "exp_mean = 1 / rate\n",
    "exp_std = 1 / rate\n",
    "\n",
    "# Simulate sampling distribution\n",
    "sample_size = 30\n",
    "n_sims = 2000\n",
    "exp_sample_means = []\n",
    "\n",
    "for _ in range(n_sims):\n",
    "    sample = np.random.choice(exp_pop, size=sample_size, replace=False)\n",
    "    exp_sample_means.append(sample.mean())\n",
    "\n",
    "exp_sample_means = np.array(exp_sample_means)\n",
    "exp_se = exp_std / np.sqrt(sample_size)\n",
    "\n",
    "print(\"‚è∞ Test 2: Exponential Distribution (Highly Skewed!)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Population: Exponential(Œª={rate})\")\n",
    "print(f\"Population mean: Œº = {exp_mean:.2f} days\")\n",
    "print(f\"Population std: œÉ = {exp_std:.3f} days\")\n",
    "print(f\"Population skewness: {stats.skew(exp_pop):.2f} (highly skewed!)\")\n",
    "print(f\"Sample size: n = {sample_size}\")\n",
    "print(f\"\\nTheoretical SE = {exp_se:.3f}\")\n",
    "print(f\"Empirical SE = {exp_sample_means.std():.3f}\")\n",
    "print(f\"Sampling dist skewness: {stats.skew(exp_sample_means):.2f} (nearly symmetric!)\")\n",
    "print(f\"\\n‚úì Sample means are approximately normal (population was exponential!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 3: Exponential ‚Üí Normal\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Exponential population\n",
    "axes[0].hist(exp_pop[:5000], bins=50, alpha=0.7, color='steelblue', \n",
    "             edgecolor='black', density=True, range=(0, 10))\n",
    "x = np.linspace(0, 10, 100)\n",
    "axes[0].plot(x, stats.expon.pdf(x, scale=1/rate), 'r-', linewidth=2, \n",
    "             label=f'Exponential(Œª={rate})')\n",
    "axes[0].axvline(exp_mean, color='black', linestyle='--', linewidth=2, \n",
    "                label=f'Œº = {exp_mean:.1f}')\n",
    "axes[0].set_xlabel('Days Between Pest Events', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('Population: EXPONENTIAL (skewed!) ‚è∞', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Normal sampling distribution\n",
    "axes[1].hist(exp_sample_means, bins=40, alpha=0.7, color='orange', \n",
    "             edgecolor='black', density=True)\n",
    "x = np.linspace(exp_sample_means.min(), exp_sample_means.max(), 100)\n",
    "axes[1].plot(x, stats.norm.pdf(x, exp_mean, exp_se), 'r-', \n",
    "             linewidth=2, label=f'N({exp_mean:.1f}, {exp_se:.3f})')\n",
    "axes[1].axvline(exp_mean, color='black', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Sample Mean (days)', fontsize=11)\n",
    "axes[1].set_ylabel('Density', fontsize=11)\n",
    "axes[1].set_title(f'Sampling Distribution: NORMAL! üîî', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('‚ú® CLT Magic: Highly Skewed ‚Üí Normal! ‚ú®', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nü§Ø INCREDIBLE! Highly skewed exponential ‚Üí Symmetric normal distribution!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèîÔ∏è Test 3: Bimodal Distribution (two peaks - very non-normal!)\n",
    "# Example: Soil types create two distinct yield groups\n",
    "\n",
    "# Create bimodal population (mixture of two normals)\n",
    "n_each = 50000\n",
    "group1 = np.random.normal(4.0, 0.5, n_each)  # Sandy soil\n",
    "group2 = np.random.normal(6.5, 0.5, n_each)  # Clay soil\n",
    "bimodal_pop = np.concatenate([group1, group2])\n",
    "bimodal_mean = bimodal_pop.mean()\n",
    "bimodal_std = bimodal_pop.std()\n",
    "\n",
    "# Simulate sampling distribution\n",
    "sample_size = 30\n",
    "n_sims = 2000\n",
    "bimodal_sample_means = []\n",
    "\n",
    "for _ in range(n_sims):\n",
    "    sample = np.random.choice(bimodal_pop, size=sample_size, replace=False)\n",
    "    bimodal_sample_means.append(sample.mean())\n",
    "\n",
    "bimodal_sample_means = np.array(bimodal_sample_means)\n",
    "bimodal_se = bimodal_std / np.sqrt(sample_size)\n",
    "\n",
    "print(\"üèîÔ∏è Test 3: Bimodal Distribution (Two Peaks!)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Population: Mixture of N(4.0, 0.5) and N(6.5, 0.5)\")\n",
    "print(f\"Population mean: Œº = {bimodal_mean:.2f} tons/hectare\")\n",
    "print(f\"Population std: œÉ = {bimodal_std:.3f} tons/hectare\")\n",
    "print(f\"Sample size: n = {sample_size}\")\n",
    "print(f\"\\nTheoretical SE = {bimodal_se:.3f}\")\n",
    "print(f\"Empirical SE = {bimodal_sample_means.std():.3f}\")\n",
    "print(f\"\\n‚úì Sample means are approximately normal (population had TWO peaks!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 4: Bimodal ‚Üí Normal\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Bimodal population\n",
    "axes[0].hist(bimodal_pop[:10000], bins=50, alpha=0.7, color='steelblue', \n",
    "             edgecolor='black', density=True)\n",
    "axes[0].axvline(4.0, color='green', linestyle=':', linewidth=1.5, alpha=0.7, \n",
    "                label='Peak 1 (sandy)')\n",
    "axes[0].axvline(6.5, color='purple', linestyle=':', linewidth=1.5, alpha=0.7, \n",
    "                label='Peak 2 (clay)')\n",
    "axes[0].axvline(bimodal_mean, color='black', linestyle='--', linewidth=2, \n",
    "                label=f'Overall Œº = {bimodal_mean:.1f}')\n",
    "axes[0].set_xlabel('Yield (tons/hectare)', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('Population: BIMODAL (two peaks!) üèîÔ∏è', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Normal sampling distribution\n",
    "axes[1].hist(bimodal_sample_means, bins=40, alpha=0.7, color='orange', \n",
    "             edgecolor='black', density=True)\n",
    "x = np.linspace(bimodal_sample_means.min(), bimodal_sample_means.max(), 100)\n",
    "axes[1].plot(x, stats.norm.pdf(x, bimodal_mean, bimodal_se), 'r-', \n",
    "             linewidth=2, label=f'N({bimodal_mean:.1f}, {bimodal_se:.3f})')\n",
    "axes[1].axvline(bimodal_mean, color='black', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Sample Mean (tons/hectare)', fontsize=11)\n",
    "axes[1].set_ylabel('Density', fontsize=11)\n",
    "axes[1].set_title(f'Sampling Distribution: NORMAL! üîî', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('‚ú® CLT Magic: Bimodal ‚Üí Normal! ‚ú®', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nü§Ø MIND-BLOWING! Two-peaked distribution ‚Üí Single-peaked normal distribution!\")\n",
    "print(\"   This is the POWER of the Central Limit Theorem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Effect of Sample Size on CLT Convergence üìè\n",
    "\n",
    "### How Large Does n Need to Be?\n",
    "\n",
    "The CLT says \"for large n\", but how large is large enough?\n",
    "\n",
    "**Rule of Thumb**: n ‚â• 30 is usually sufficient\n",
    "\n",
    "But it depends on the population:\n",
    "- **Normal population**: n = 2 is enough! (already normal)\n",
    "- **Symmetric population**: n = 10-15 often sufficient\n",
    "- **Skewed population**: n = 30-50 needed\n",
    "- **Heavily skewed/extreme**: n > 50 might be required\n",
    "\n",
    "Let's see convergence in action with the exponential distribution (heavily skewed)!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìè Demonstrate convergence with different sample sizes\n",
    "# Use exponential (skewed) population\n",
    "\n",
    "sample_sizes = [2, 5, 10, 20, 30, 50]\n",
    "n_sims = 2000\n",
    "\n",
    "# Store sampling distributions for each n\n",
    "convergence_results = {}\n",
    "\n",
    "for n in sample_sizes:\n",
    "    means = []\n",
    "    for _ in range(n_sims):\n",
    "        sample = np.random.choice(exp_pop, size=n, replace=False)\n",
    "        means.append(sample.mean())\n",
    "    convergence_results[n] = np.array(means)\n",
    "\n",
    "print(\"üìè Convergence to Normality (Exponential Population):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'n':<8} {'Mean':<10} {'Std':<10} {'Skewness':<12} {'Normality'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for n in sample_sizes:\n",
    "    means_array = convergence_results[n]\n",
    "    skewness = stats.skew(means_array)\n",
    "    # Shapiro-Wilk test (p > 0.05 ‚Üí approximately normal)\n",
    "    _, p_value = stats.shapiro(means_array[:5000] if len(means_array) > 5000 else means_array)\n",
    "    \n",
    "    normality = \"‚úì Normal\" if p_value > 0.05 else \"‚ö†Ô∏è Not yet\"\n",
    "    \n",
    "    print(f\"{n:<8} {means_array.mean():<10.3f} {means_array.std():<10.3f} \"\n",
    "          f\"{skewness:<12.3f} {normality}\")\n",
    "\n",
    "print(\"\\nüí° Notice:\")\n",
    "print(\"   - Skewness decreases as n increases\")\n",
    "print(\"   - By n=30, distribution is approximately normal\")\n",
    "print(\"   - Standard deviation (SE) decreases with ‚àön\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 5: Convergence grid (6 panels)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "fig.suptitle('CLT Convergence: Sample Size Effect (Exponential Population) üìè', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, n in enumerate(sample_sizes):\n",
    "    ax = axes[idx]\n",
    "    means_array = convergence_results[n]\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(means_array, bins=30, alpha=0.7, color='steelblue', \n",
    "            edgecolor='black', density=True)\n",
    "    \n",
    "    # Overlay theoretical normal\n",
    "    se = exp_std / np.sqrt(n)\n",
    "    x = np.linspace(means_array.min(), means_array.max(), 100)\n",
    "    ax.plot(x, stats.norm.pdf(x, exp_mean, se), 'r-', linewidth=2, \n",
    "            label='Theoretical N')\n",
    "    \n",
    "    # Mark mean\n",
    "    ax.axvline(exp_mean, color='black', linestyle='--', linewidth=1.5)\n",
    "    \n",
    "    # Calculate skewness\n",
    "    skewness = stats.skew(means_array)\n",
    "    \n",
    "    # Status indicator\n",
    "    status = \"‚úì Normal\" if abs(skewness) < 0.5 else \"‚ö†Ô∏è Converging\"\n",
    "    color = 'lightgreen' if abs(skewness) < 0.5 else 'lightyellow'\n",
    "    \n",
    "    textstr = f'n = {n}\\nSE = {se:.3f}\\nSkew = {skewness:.2f}\\n{status}'\n",
    "    props = dict(boxstyle='round', facecolor=color, alpha=0.8)\n",
    "    ax.text(0.65, 0.95, textstr, transform=ax.transAxes, fontsize=9,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    \n",
    "    ax.set_xlabel('Sample Mean', fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "    ax.set_title(f'n = {n}', fontsize=11, fontweight='bold')\n",
    "    ax.legend(fontsize=8, loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observation:\")\n",
    "print(\"   - n=2: Still very skewed (like population)\")\n",
    "print(\"   - n=10: Starting to look normal\")\n",
    "print(\"   - n=30: Clearly normal! (the rule of thumb)\")\n",
    "print(\"   - n=50: Even more normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Why CLT Matters: Practical Applications üéØ\n",
    "\n",
    "### The Power of CLT:\n",
    "\n",
    "1. **Enables Statistical Inference**\n",
    "   - We can construct confidence intervals\n",
    "   - We can perform hypothesis tests\n",
    "   - All based on normal distribution properties\n",
    "\n",
    "2. **Works with Real (Non-Normal) Data**\n",
    "   - Real agricultural data is rarely normal\n",
    "   - But sample means ARE approximately normal\n",
    "   - So we can still use normal-based methods!\n",
    "\n",
    "3. **Provides Predictable Uncertainty**\n",
    "   - SE = œÉ/‚àön (simple formula)\n",
    "   - Larger samples ‚Üí smaller SE ‚Üí more precision\n",
    "   - We can plan required sample size\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåæ Real-world application: Skewed agricultural data\n",
    "# Example: Disease incidence is often skewed (many fields with low incidence, few with high)\n",
    "\n",
    "# Create realistic skewed data (Gamma distribution)\n",
    "shape, scale = 2.0, 1.5\n",
    "disease_pop = np.random.gamma(shape, scale, 100000)\n",
    "disease_mean = shape * scale\n",
    "disease_std = np.sqrt(shape * scale**2)\n",
    "\n",
    "# Sample and create sampling distribution\n",
    "sample_size = 40\n",
    "n_sims = 2000\n",
    "disease_sample_means = []\n",
    "\n",
    "for _ in range(n_sims):\n",
    "    sample = np.random.choice(disease_pop, size=sample_size, replace=False)\n",
    "    disease_sample_means.append(sample.mean())\n",
    "\n",
    "disease_sample_means = np.array(disease_sample_means)\n",
    "disease_se = disease_std / np.sqrt(sample_size)\n",
    "\n",
    "print(\"üåæ Real Agricultural Example: Disease Incidence\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Population: Skewed (Gamma distribution)\")\n",
    "print(f\"Population mean: Œº = {disease_mean:.2f} infected plants per field\")\n",
    "print(f\"Population std: œÉ = {disease_std:.3f}\")\n",
    "print(f\"Population skewness: {stats.skew(disease_pop):.2f} (skewed!)\")\n",
    "print(f\"\\nSample size: n = {sample_size}\")\n",
    "print(f\"SE = {disease_se:.3f}\")\n",
    "print(f\"\\nSampling distribution mean: {disease_sample_means.mean():.3f}\")\n",
    "print(f\"Sampling distribution std: {disease_sample_means.std():.3f}\")\n",
    "print(f\"Sampling distribution skewness: {stats.skew(disease_sample_means):.2f} (nearly normal!)\")\n",
    "print(f\"\\n‚úì Despite skewed population, sample means are approximately normal!\")\n",
    "print(\"  ‚Üí We can use normal-based inference methods!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 6: Real skewed data application\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Skewed population\n",
    "axes[0].hist(disease_pop[:5000], bins=50, alpha=0.7, color='steelblue', \n",
    "             edgecolor='black', density=True)\n",
    "axes[0].axvline(disease_mean, color='black', linestyle='--', linewidth=2, \n",
    "                label=f'Œº = {disease_mean:.1f}')\n",
    "axes[0].set_xlabel('Disease Incidence (infected plants)', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('Population: SKEWED üåæ', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].text(0.65, 0.95, f'Skewness = {stats.skew(disease_pop):.2f}',\n",
    "             transform=axes[0].transAxes, fontsize=10,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# Right: Normal sampling distribution\n",
    "axes[1].hist(disease_sample_means, bins=40, alpha=0.7, color='orange', \n",
    "             edgecolor='black', density=True)\n",
    "x = np.linspace(disease_sample_means.min(), disease_sample_means.max(), 100)\n",
    "axes[1].plot(x, stats.norm.pdf(x, disease_mean, disease_se), 'r-', \n",
    "             linewidth=2, label=f'N({disease_mean:.1f}, {disease_se:.3f})')\n",
    "axes[1].axvline(disease_mean, color='black', linestyle='--', linewidth=2)\n",
    "\n",
    "# Add 95% interval\n",
    "lower = disease_mean - 1.96 * disease_se\n",
    "upper = disease_mean + 1.96 * disease_se\n",
    "axes[1].axvspan(lower, upper, alpha=0.2, color='green', \n",
    "                label='95% of sample means')\n",
    "\n",
    "axes[1].set_xlabel('Sample Mean (infected plants)', fontsize=11)\n",
    "axes[1].set_ylabel('Density', fontsize=11)\n",
    "axes[1].set_title(f'Sampling Distribution: NORMAL! üîî', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].text(0.55, 0.95, f'Skewness = {stats.skew(disease_sample_means):.2f}',\n",
    "             transform=axes[1].transAxes, fontsize=10,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "plt.suptitle('CLT with Real Agricultural Data üåæ', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Practical Implication:\")\n",
    "print(f\"   - If we take a sample of {sample_size} fields:\")\n",
    "print(f\"   - 95% of the time, our sample mean will be within\")\n",
    "print(f\"     [{lower:.2f}, {upper:.2f}] infected plants\")\n",
    "print(\"   - This lets us quantify uncertainty even with skewed data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Machine Learning Connection ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "### Why CLT is Fundamental to ML\n",
    "\n",
    "The Central Limit Theorem explains why many ML techniques work:\n",
    "\n",
    "#### 1. Bootstrap and Bagging üéí\n",
    "- **Bootstrap**: Resample data many times, calculate statistic each time\n",
    "- **Aggregating**: Average the statistics\n",
    "- **Why it works**: CLT says the average will be approximately normal with reduced variance!\n",
    "\n",
    "#### 2. Ensemble Methods üéØ\n",
    "- **Random Forests**: Average predictions from many decision trees\n",
    "- **Bagging**: Train model on bootstrap samples, average predictions\n",
    "- **Why it works**: Averaging reduces variance (CLT!)\n",
    "\n",
    "#### 3. Model Averaging üìä\n",
    "- Train multiple models, average their predictions\n",
    "- **Why it works**: Even if individual models have errors, averaging tends toward truth\n",
    "- CLT says errors will cancel out when averaged!\n",
    "\n",
    "#### 4. Cross-Validation üîÑ\n",
    "- Multiple train/test splits ‚Üí multiple accuracy scores\n",
    "- Average score is approximately normal (CLT!)\n",
    "- Can construct confidence intervals for true performance\n",
    "\n",
    "### The Key Insight:\n",
    "\n",
    "**Averaging reduces variance and tends toward the true value!**\n",
    "\n",
    "This is why:\n",
    "- üå≤ Random Forest > Single Decision Tree\n",
    "- üì¶ Bagging improves unstable models\n",
    "- üéØ Ensemble methods win competitions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ ML Demo: Why Bootstrap Aggregating (Bagging) Works\n",
    "# Simulate predictions from unstable models\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# True value we're trying to predict\n",
    "true_yield = 5.2\n",
    "\n",
    "# Single model predictions: high variance (unstable model)\n",
    "# Each prediction has error ~N(0, 0.8)\n",
    "n_models = 100\n",
    "single_predictions = true_yield + np.random.normal(0, 0.8, n_models)\n",
    "\n",
    "# Bagging: Average predictions from bootstrap samples\n",
    "# Simulate by taking means of different subsets\n",
    "n_bagged = 1000\n",
    "bagged_predictions = []\n",
    "\n",
    "for _ in range(n_bagged):\n",
    "    # Bootstrap sample: sample with replacement\n",
    "    bootstrap_sample = np.random.choice(single_predictions, \n",
    "                                        size=20, replace=True)\n",
    "    # Average (aggregate)\n",
    "    bagged_predictions.append(bootstrap_sample.mean())\n",
    "\n",
    "bagged_predictions = np.array(bagged_predictions)\n",
    "\n",
    "print(\"ü§ñ Bootstrap Aggregating (Bagging) Demonstration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"True yield to predict: {true_yield:.1f} tons/hectare\")\n",
    "print(f\"\\nSingle Unstable Model:\")\n",
    "print(f\"  Prediction error std: {single_predictions.std():.3f}\")\n",
    "print(f\"  Mean squared error: {((single_predictions - true_yield)**2).mean():.3f}\")\n",
    "print(f\"\\nBagged Model (average of 20):\")\n",
    "print(f\"  Prediction error std: {bagged_predictions.std():.3f}\")\n",
    "print(f\"  Mean squared error: {((bagged_predictions - true_yield)**2).mean():.3f}\")\n",
    "print(f\"\\n‚úì Variance reduced by: {(1 - bagged_predictions.std()/single_predictions.std())*100:.1f}%\")\n",
    "print(\"\\nüí° This is the Central Limit Theorem in action!\")\n",
    "print(\"   Averaging ‚Üí Reduced variance ‚Üí Better predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 7: Bagging reduces variance (CLT in action!)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: Single model predictions\n",
    "axes[0].hist(single_predictions, bins=30, alpha=0.7, color='red', \n",
    "             edgecolor='black', density=True)\n",
    "axes[0].axvline(true_yield, color='black', linestyle='--', linewidth=2, \n",
    "                label=f'True value = {true_yield}')\n",
    "axes[0].axvline(single_predictions.mean(), color='blue', linestyle='-', linewidth=2,\n",
    "                label=f'Mean prediction = {single_predictions.mean():.2f}')\n",
    "axes[0].set_xlabel('Predicted Yield (tons/hectare)', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('Single Unstable Model üé≤', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].text(0.55, 0.95, f'Std = {single_predictions.std():.3f}\\nHigh Variance!',\n",
    "             transform=axes[0].transAxes, fontsize=10,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.8))\n",
    "\n",
    "# Right: Bagged predictions\n",
    "axes[1].hist(bagged_predictions, bins=30, alpha=0.7, color='green', \n",
    "             edgecolor='black', density=True)\n",
    "axes[1].axvline(true_yield, color='black', linestyle='--', linewidth=2, \n",
    "                label=f'True value = {true_yield}')\n",
    "axes[1].axvline(bagged_predictions.mean(), color='blue', linestyle='-', linewidth=2,\n",
    "                label=f'Mean prediction = {bagged_predictions.mean():.2f}')\n",
    "\n",
    "# Overlay theoretical normal (CLT!)\n",
    "x = np.linspace(bagged_predictions.min(), bagged_predictions.max(), 100)\n",
    "theoretical_se = single_predictions.std() / np.sqrt(20)\n",
    "axes[1].plot(x, stats.norm.pdf(x, true_yield, theoretical_se), 'r-', \n",
    "             linewidth=2, alpha=0.7, label='CLT prediction')\n",
    "\n",
    "axes[1].set_xlabel('Predicted Yield (tons/hectare)', fontsize=11)\n",
    "axes[1].set_ylabel('Density', fontsize=11)\n",
    "axes[1].set_title('Bagged Model (Average of 20) ‚úÖ', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].text(0.55, 0.95, f'Std = {bagged_predictions.std():.3f}\\nLow Variance!',\n",
    "             transform=axes[1].transAxes, fontsize=10,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "plt.suptitle('üéØ Central Limit Theorem ‚Üí Why Bagging Works! üéØ', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key ML Insights:\")\n",
    "print(\"   1. Single model: High variance, unreliable\")\n",
    "print(\"   2. Bagged model: Lower variance (CLT!), more reliable\")\n",
    "print(\"   3. Distribution becomes normal (CLT prediction)\")\n",
    "print(\"   4. Closer to true value on average\")\n",
    "print(\"\\nüéØ This is why:\")\n",
    "print(\"   - Random Forests work (bagging decision trees)\")\n",
    "print(\"   - Ensemble methods win competitions\")\n",
    "print(\"   - Averaging models improves performance\")\n",
    "print(\"\\n‚ú® The Central Limit Theorem makes all of this possible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways üéØ\n",
    "\n",
    "### The Central Limit Theorem:\n",
    "\n",
    "1. ‚úÖ **Universal Normality** ‚≠ê‚≠ê:\n",
    "   - Sample means are approximately normal for large n\n",
    "   - **Works for ANY population distribution!**\n",
    "   - Uniform ‚Üí Normal, Skewed ‚Üí Normal, Bimodal ‚Üí Normal\n",
    "\n",
    "2. ‚úÖ **Sampling Distribution**:\n",
    "   - Centered at population mean: E[XÃÑ] = Œº\n",
    "   - Spread is standard error: SD[XÃÑ] = œÉ/‚àön\n",
    "   - Approximately: XÃÑ ~ N(Œº, œÉ¬≤/n)\n",
    "\n",
    "3. ‚úÖ **Sample Size Matters**:\n",
    "   - Rule of thumb: n ‚â• 30 usually sufficient\n",
    "   - More skewed population ‚Üí larger n needed\n",
    "   - Normal population ‚Üí any n works\n",
    "\n",
    "4. ‚úÖ **Enables Inference**:\n",
    "   - Can use normal distribution tools\n",
    "   - Construct confidence intervals\n",
    "   - Perform hypothesis tests\n",
    "   - All without knowing population distribution!\n",
    "\n",
    "5. ‚úÖ **ML Foundation** ‚≠ê‚≠ê:\n",
    "   - Bootstrap works because of CLT\n",
    "   - Bagging reduces variance (CLT!)\n",
    "   - Ensemble methods average predictions (CLT!)\n",
    "   - Random Forests = Bootstrap + Trees + CLT\n",
    "\n",
    "### The Magic Formula:\n",
    "\n",
    "$$\n",
    "\\boxed{\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\text{ for large } n}\n",
    "$$\n",
    "\n",
    "**This single theorem is the foundation of:**\n",
    "- All of statistical inference\n",
    "- Confidence intervals\n",
    "- Hypothesis testing  \n",
    "- Bootstrap methods\n",
    "- Ensemble ML methods\n",
    "\n",
    "### Why It's Called \"The Most Important Theorem\":\n",
    "\n",
    "‚ú® **It transforms the complex into the simple**\n",
    "‚ú® **It makes inference possible**  \n",
    "‚ú® **It works universally**\n",
    "‚ú® **It powers modern ML**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps üöÄ\n",
    "\n",
    "**Coming Up Next: Point Estimation and Maximum Likelihood Estimation (MLE)** ‚≠ê\n",
    "\n",
    "Now that we understand sampling distributions and the CLT, we're ready to tackle:\n",
    "\n",
    "- **Point Estimation**: How to estimate population parameters\n",
    "- **Properties of Estimators**: Unbiased, consistent, efficient\n",
    "- **Maximum Likelihood Estimation (MLE)**: The gold standard ‚≠ê\n",
    "- **ML Connection**: Training is parameter estimation!\n",
    "\n",
    "**Critical Insight**: Every time you train an ML model, you're doing MLE!\n",
    "- Linear regression ‚Üí MLE\n",
    "- Logistic regression ‚Üí MLE  \n",
    "- Neural networks ‚Üí MLE\n",
    "\n",
    "Understanding estimation helps you understand what ML training actually does.\n",
    "\n",
    "See you in **`03_point_estimation.ipynb`**!\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work! You now understand the most important theorem in statistics and why ensemble methods work!** üéØ‚ú®üåæ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
