{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Estimation: From Samples to Parameters üéØ\n",
    "\n",
    "## Introduction: Making Our Best Guess\n",
    "\n",
    "In the previous notebooks, we learned about:\n",
    "- **Sampling**: Taking subsets from populations\n",
    "- **Sampling distributions**: How sample statistics vary\n",
    "- **Central Limit Theorem**: Why sample means are normal\n",
    "\n",
    "But we haven't answered the fundamental question: **How do we actually estimate population parameters from our sample?**\n",
    "\n",
    "### The Setup:\n",
    "\n",
    "- üåç **Population**: Has unknown parameter Œ∏ (could be Œº, œÉ¬≤, p, etc.)\n",
    "- üìä **Sample**: We observe data x‚ÇÅ, x‚ÇÇ, ..., x‚Çô\n",
    "- üéØ **Goal**: Estimate Œ∏ using our sample\n",
    "\n",
    "### Real Example:\n",
    "\n",
    "You sample 50 wheat fields and measure their yields. The average is 5.15 tons/hectare.\n",
    "\n",
    "**Questions**:\n",
    "- Is 5.15 our best estimate of the true population mean?\n",
    "- How do we know it's a \"good\" estimate?\n",
    "- Are there better ways to estimate the mean?\n",
    "\n",
    "This is **point estimation**!\n",
    "\n",
    "### ML Connection ü§ñ\n",
    "\n",
    "**Training an ML model IS parameter estimation!**\n",
    "- Linear regression: Estimating slope and intercept\n",
    "- Logistic regression: Estimating coefficients\n",
    "- Neural networks: Estimating millions of weights\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE)** is the foundation of most ML training algorithms!\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. ‚úÖ Understand **point estimation** concept and terminology\n",
    "2. ‚úÖ Learn properties of good estimators (unbiased, consistent, efficient)\n",
    "3. ‚úÖ Master **Maximum Likelihood Estimation (MLE)** ‚≠ê‚≠ê\n",
    "4. ‚úÖ Understand **Method of Moments** estimation\n",
    "5. ‚úÖ Connect estimation to **ML model training** ‚≠ê‚≠ê\n",
    "6. ‚úÖ Implement MLE from scratch for common distributions\n",
    "\n",
    "‚≠ê‚≠ê = Most critical concept\n",
    "\n",
    "---\n",
    "\n",
    "Let's learn how to estimate! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Setup: Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Set style for beautiful plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Setup complete!\")\n",
    "print(\"üéØ Ready to learn point estimation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Point Estimation Basics üìä\n",
    "\n",
    "### Key Terminology:\n",
    "\n",
    "**Estimator (Œ∏ÃÇ)**: A rule/formula for calculating an estimate from sample data\n",
    "- Example: Sample mean XÃÑ = (1/n)Œ£x·µ¢ is an estimator\n",
    "- An estimator is a **random variable** (changes with different samples)\n",
    "\n",
    "**Estimate**: The actual numerical value from a specific sample\n",
    "- Example: xÃÑ = 5.15 tons/hectare is an estimate\n",
    "- An estimate is a **fixed number**\n",
    "\n",
    "**Parameter (Œ∏)**: The true population value we're trying to estimate\n",
    "- Example: Œº = true population mean (unknown)\n",
    "\n",
    "### Common Estimators:\n",
    "\n",
    "| Parameter | Estimator | Formula |\n",
    "|-----------|-----------|----------|\n",
    "| Population mean Œº | Sample mean XÃÑ | (1/n)Œ£x·µ¢ |\n",
    "| Population variance œÉ¬≤ | Sample variance s¬≤ | (1/(n-1))Œ£(x·µ¢ - xÃÑ)¬≤ |\n",
    "| Population proportion p | Sample proportion pÃÇ | (# successes)/n |\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "# Sample data\n",
    "yields = [5.1, 5.3, 4.9, 5.2, 5.4]\n",
    "\n",
    "# Estimator: Sample mean\n",
    "estimate = np.mean(yields)  # 5.18\n",
    "```\n",
    "\n",
    "- **Estimator**: np.mean() function (the rule)\n",
    "- **Estimate**: 5.18 (the result)\n",
    "- **Parameter**: Œº (unknown true mean)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåæ Example: Estimate population mean and variance from a sample\n",
    "\n",
    "# True population (unknown to us in practice)\n",
    "true_mean = 5.2\n",
    "true_std = 0.8\n",
    "true_variance = true_std ** 2\n",
    "\n",
    "# Generate population\n",
    "population = np.random.normal(true_mean, true_std, 100000)\n",
    "\n",
    "# Take a sample (what we actually observe)\n",
    "sample_size = 50\n",
    "sample = np.random.choice(population, size=sample_size, replace=False)\n",
    "\n",
    "# Calculate estimates\n",
    "mean_estimate = sample.mean()\n",
    "variance_estimate = sample.var(ddof=1)  # ddof=1 for unbiased estimate\n",
    "std_estimate = np.sqrt(variance_estimate)\n",
    "\n",
    "print(\"üéØ Point Estimation Example:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"TRUE POPULATION PARAMETERS (unknown in practice):\")\n",
    "print(f\"  Œº (mean) = {true_mean:.3f} tons/hectare\")\n",
    "print(f\"  œÉ¬≤ (variance) = {true_variance:.3f}\")\n",
    "print(f\"  œÉ (std dev) = {true_std:.3f}\")\n",
    "print(f\"\\nSAMPLE DATA (n={sample_size}):\")\n",
    "print(f\"  Observed yields: {sample[:5].round(2)}... (showing first 5)\")\n",
    "print(f\"\\nPOINT ESTIMATES:\")\n",
    "print(f\"  ŒºÃÇ (estimated mean) = {mean_estimate:.3f} tons/hectare\")\n",
    "print(f\"  œÉÃÇ¬≤ (estimated variance) = {variance_estimate:.3f}\")\n",
    "print(f\"  œÉÃÇ (estimated std dev) = {std_estimate:.3f}\")\n",
    "print(f\"\\nESTIMATION ERRORS:\")\n",
    "print(f\"  Mean error: {abs(mean_estimate - true_mean):.3f}\")\n",
    "print(f\"  Variance error: {abs(variance_estimate - true_variance):.3f}\")\n",
    "print(f\"\\nüí° Our estimates are close but not perfect (sampling variability!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 1: Point estimate on distribution\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot population distribution\n",
    "x = np.linspace(population.min(), population.max(), 100)\n",
    "plt.plot(x, stats.norm.pdf(x, true_mean, true_std), 'b-', linewidth=3, \n",
    "         alpha=0.5, label='True Population Distribution')\n",
    "\n",
    "# Plot sample histogram\n",
    "plt.hist(sample, bins=15, alpha=0.6, color='steelblue', edgecolor='black', \n",
    "         density=True, label=f'Sample (n={sample_size})')\n",
    "\n",
    "# Mark true parameter\n",
    "plt.axvline(true_mean, color='blue', linestyle='--', linewidth=2, \n",
    "            label=f'True Œº = {true_mean:.2f}')\n",
    "\n",
    "# Mark estimate\n",
    "plt.axvline(mean_estimate, color='red', linestyle='-', linewidth=2, \n",
    "            label=f'Estimate ŒºÃÇ = {mean_estimate:.2f}')\n",
    "\n",
    "plt.xlabel('Wheat Yield (tons/hectare)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('Point Estimation: Sample Mean as Estimate of Population Mean üéØ', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° The sample mean (red line) is our point estimate of the true mean (blue line)\")\n",
    "print(\"   It's close, but not exact due to sampling variability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Properties of Good Estimators ‚úÖ\n",
    "\n",
    "Not all estimators are created equal! We want estimators that have desirable properties:\n",
    "\n",
    "### 1. Unbiased ‚≠ê\n",
    "\n",
    "**Definition**: An estimator Œ∏ÃÇ is unbiased if E[Œ∏ÃÇ] = Œ∏\n",
    "\n",
    "In words: On average across many samples, the estimator equals the true parameter\n",
    "\n",
    "$$\n",
    "\\text{Bias} = E[\\hat{\\theta}] - \\theta\n",
    "$$\n",
    "\n",
    "**Examples**:\n",
    "- Sample mean XÃÑ is unbiased for Œº: E[XÃÑ] = Œº ‚úì\n",
    "- Sample variance s¬≤ (with ddof=1) is unbiased for œÉ¬≤ ‚úì\n",
    "- Sample variance (with ddof=0) is biased! ‚úó\n",
    "\n",
    "### 2. Consistent ‚≠ê\n",
    "\n",
    "**Definition**: As n ‚Üí ‚àû, Œ∏ÃÇ ‚Üí Œ∏ (converges to true value)\n",
    "\n",
    "In words: With more data, the estimator gets arbitrarily close to the truth\n",
    "\n",
    "### 3. Efficient ‚≠ê\n",
    "\n",
    "**Definition**: Among unbiased estimators, has the smallest variance\n",
    "\n",
    "In words: Most precise estimator (tightest sampling distribution)\n",
    "\n",
    "### Mean Squared Error (MSE):\n",
    "\n",
    "Combines bias and variance:\n",
    "\n",
    "$$\n",
    "MSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2] = \\text{Bias}^2 + \\text{Variance}\n",
    "$$\n",
    "\n",
    "**Goal**: Minimize MSE (trade-off between bias and variance)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ Demonstrate unbiasedness\n",
    "# Take many samples, show that average of estimates equals true parameter\n",
    "\n",
    "n_simulations = 1000\n",
    "sample_size = 50\n",
    "\n",
    "# Store estimates from each sample\n",
    "mean_estimates = []\n",
    "var_estimates_biased = []  # ddof=0 (biased)\n",
    "var_estimates_unbiased = []  # ddof=1 (unbiased)\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    sample = np.random.choice(population, size=sample_size, replace=False)\n",
    "    mean_estimates.append(sample.mean())\n",
    "    var_estimates_biased.append(sample.var(ddof=0))\n",
    "    var_estimates_unbiased.append(sample.var(ddof=1))\n",
    "\n",
    "mean_estimates = np.array(mean_estimates)\n",
    "var_estimates_biased = np.array(var_estimates_biased)\n",
    "var_estimates_unbiased = np.array(var_estimates_unbiased)\n",
    "\n",
    "print(\"üî¨ Demonstrating Unbiasedness:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Simulation: {n_simulations} samples of size n={sample_size}\")\n",
    "print(f\"\\n1. SAMPLE MEAN (estimator for Œº):\")\n",
    "print(f\"   True Œº = {true_mean:.4f}\")\n",
    "print(f\"   Average of {n_simulations} estimates = {mean_estimates.mean():.4f}\")\n",
    "print(f\"   Bias = {mean_estimates.mean() - true_mean:.4f} ‚úì UNBIASED\")\n",
    "\n",
    "print(f\"\\n2. SAMPLE VARIANCE with ddof=0 (biased):\")\n",
    "print(f\"   True œÉ¬≤ = {true_variance:.4f}\")\n",
    "print(f\"   Average of {n_simulations} estimates = {var_estimates_biased.mean():.4f}\")\n",
    "print(f\"   Bias = {var_estimates_biased.mean() - true_variance:.4f} ‚úó BIASED (underestimates)\")\n",
    "\n",
    "print(f\"\\n3. SAMPLE VARIANCE with ddof=1 (unbiased):\")\n",
    "print(f\"   True œÉ¬≤ = {true_variance:.4f}\")\n",
    "print(f\"   Average of {n_simulations} estimates = {var_estimates_unbiased.mean():.4f}\")\n",
    "print(f\"   Bias = {var_estimates_unbiased.mean() - true_variance:.4f} ‚úì UNBIASED\")\n",
    "\n",
    "print(f\"\\nüí° Unbiased estimator: E[Œ∏ÃÇ] = Œ∏ (average equals true value)\")\n",
    "print(f\"   This is why we use ddof=1 for sample variance!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 2: Sampling distributions showing unbiasedness\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Mean estimates (unbiased)\n",
    "axes[0].hist(mean_estimates, bins=40, alpha=0.7, color='green', \n",
    "             edgecolor='black', density=True)\n",
    "axes[0].axvline(true_mean, color='black', linestyle='--', linewidth=2, \n",
    "                label=f'True Œº = {true_mean:.2f}')\n",
    "axes[0].axvline(mean_estimates.mean(), color='red', linestyle='-', linewidth=2,\n",
    "                label=f'E[ŒºÃÇ] = {mean_estimates.mean():.2f}')\n",
    "axes[0].set_xlabel('Estimate of Œº', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('Sample Mean: UNBIASED ‚úì', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].text(0.55, 0.95, 'Centered at\\ntrue value!',\n",
    "             transform=axes[0].transAxes, fontsize=10,\n",
    "             verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8))\n",
    "\n",
    "# Right: Variance estimates (biased vs unbiased)\n",
    "axes[1].hist(var_estimates_biased, bins=40, alpha=0.5, color='red', \n",
    "             edgecolor='black', density=True, label=f'ddof=0 (biased)')\n",
    "axes[1].hist(var_estimates_unbiased, bins=40, alpha=0.5, color='green', \n",
    "             edgecolor='black', density=True, label=f'ddof=1 (unbiased)')\n",
    "axes[1].axvline(true_variance, color='black', linestyle='--', linewidth=2, \n",
    "                label=f'True œÉ¬≤ = {true_variance:.2f}')\n",
    "axes[1].axvline(var_estimates_biased.mean(), color='darkred', linestyle=':', \n",
    "                linewidth=1.5, alpha=0.7)\n",
    "axes[1].axvline(var_estimates_unbiased.mean(), color='darkgreen', linestyle=':', \n",
    "                linewidth=1.5, alpha=0.7)\n",
    "axes[1].set_xlabel('Estimate of œÉ¬≤', fontsize=11)\n",
    "axes[1].set_ylabel('Density', fontsize=11)\n",
    "axes[1].set_title('Sample Variance: Effect of ddof', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Unbiasedness: Does E[Œ∏ÃÇ] = Œ∏? üéØ', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observation:\")\n",
    "print(\"   - Green distribution (ddof=1) is centered at true œÉ¬≤ ‚úì\")\n",
    "print(\"   - Red distribution (ddof=0) is shifted left (underestimates) ‚úó\")\n",
    "print(\"   - Always use ddof=1 for unbiased variance estimation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìè Demonstrate consistency\n",
    "# Show estimates get closer to truth as n increases\n",
    "\n",
    "sample_sizes = [10, 25, 50, 100, 200, 500]\n",
    "n_sims = 500\n",
    "\n",
    "results = {}\n",
    "\n",
    "for n in sample_sizes:\n",
    "    estimates = []\n",
    "    for _ in range(n_sims):\n",
    "        sample = np.random.choice(population, size=n, replace=False)\n",
    "        estimates.append(sample.mean())\n",
    "    results[n] = np.array(estimates)\n",
    "\n",
    "print(\"üìè Demonstrating Consistency:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"True Œº = {true_mean:.4f}\")\n",
    "print(f\"\\n{'Sample Size':<12} {'Mean of Estimates':<20} {'Std of Estimates':<20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for n in sample_sizes:\n",
    "    mean_est = results[n].mean()\n",
    "    std_est = results[n].std()\n",
    "    print(f\"{n:<12} {mean_est:<20.4f} {std_est:<20.4f}\")\n",
    "\n",
    "print(\"\\nüí° Notice:\")\n",
    "print(\"   - As n increases, estimates cluster tighter around true value\")\n",
    "print(\"   - Standard deviation decreases (proportional to 1/‚àön)\")\n",
    "print(\"   - This is CONSISTENCY: estimator ‚Üí true value as n ‚Üí ‚àû\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 3: Consistency (distributions get narrower)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "fig.suptitle('Consistency: Estimates Converge as n Increases üìè', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, n in enumerate(sample_sizes):\n",
    "    ax = axes[idx]\n",
    "    estimates = results[n]\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(estimates, bins=30, alpha=0.7, color='steelblue', \n",
    "            edgecolor='black', density=True)\n",
    "    \n",
    "    # Mark true value\n",
    "    ax.axvline(true_mean, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'True Œº = {true_mean:.2f}')\n",
    "    \n",
    "    # Mark mean of estimates\n",
    "    ax.axvline(estimates.mean(), color='green', linestyle='-', linewidth=1.5,\n",
    "               alpha=0.7, label=f'E[ŒºÃÇ] = {estimates.mean():.2f}')\n",
    "    \n",
    "    # Statistics box\n",
    "    textstr = f'n = {n}\\nStd = {estimates.std():.3f}\\nRange = {estimates.max()-estimates.min():.3f}'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "    ax.text(0.65, 0.95, textstr, transform=ax.transAxes, fontsize=9,\n",
    "            verticalalignment='top', bbox=props)\n",
    "    \n",
    "    ax.set_xlabel('Estimate', fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "    ax.set_title(f'Sample Size n = {n}', fontsize=11, fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Same x-axis for comparison\n",
    "    ax.set_xlim(4.6, 5.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Consistency Visualized:\")\n",
    "print(\"   - Small n: Wide distribution (high variability)\")\n",
    "print(\"   - Large n: Narrow distribution (low variability)\")\n",
    "print(\"   - All centered at true value (unbiased + consistent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Maximum Likelihood Estimation (MLE) ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "### The Gold Standard of Estimation\n",
    "\n",
    "**Idea**: Choose the parameter value that makes the observed data most likely\n",
    "\n",
    "### Likelihood Function:\n",
    "\n",
    "For data x = (x‚ÇÅ, x‚ÇÇ, ..., x‚Çô) and parameter Œ∏:\n",
    "\n",
    "$$\n",
    "L(\\theta | x) = \\prod_{i=1}^{n} f(x_i | \\theta)\n",
    "$$\n",
    "\n",
    "Where f(x·µ¢ | Œ∏) is the probability density/mass function\n",
    "\n",
    "### Log-Likelihood:\n",
    "\n",
    "For computational reasons, we maximize the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta | x) = \\ln L(\\theta | x) = \\sum_{i=1}^{n} \\ln f(x_i | \\theta)\n",
    "$$\n",
    "\n",
    "### MLE Principle:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} L(\\theta | x) = \\arg\\max_{\\theta} \\ell(\\theta | x)\n",
    "$$\n",
    "\n",
    "### Why MLE is Great:\n",
    "\n",
    "1. ‚úÖ **Consistent**: Œ∏ÃÇ‚Çò‚Çó‚Çë ‚Üí Œ∏ as n ‚Üí ‚àû\n",
    "2. ‚úÖ **Asymptotically efficient**: Lowest variance for large n\n",
    "3. ‚úÖ **Asymptotically normal**: Œ∏ÃÇ‚Çò‚Çó‚Çë ~ N(Œ∏, ...) for large n\n",
    "4. ‚úÖ **Invariant**: If Œ∏ÃÇ is MLE for Œ∏, then g(Œ∏ÃÇ) is MLE for g(Œ∏)\n",
    "\n",
    "### ML Connection:\n",
    "\n",
    "**Training is MLE!**\n",
    "- Linear regression loss ‚Üí Negative log-likelihood\n",
    "- Cross-entropy loss ‚Üí Negative log-likelihood\n",
    "- Most ML training = finding MLE!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ MLE Example 1: Normal Distribution\n",
    "# Given data, find Œº and œÉ that maximize likelihood\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(42)\n",
    "true_mu = 5.2\n",
    "true_sigma = 0.8\n",
    "n = 50\n",
    "data = np.random.normal(true_mu, true_sigma, n)\n",
    "\n",
    "# MLE for normal distribution (closed-form solution)\n",
    "mu_mle = data.mean()\n",
    "sigma_mle = np.sqrt(((data - mu_mle)**2).sum() / n)  # MLE uses n, not n-1!\n",
    "\n",
    "print(\"üéØ MLE for Normal Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sample size: n = {n}\")\n",
    "print(f\"\\nTRUE PARAMETERS:\")\n",
    "print(f\"  Œº = {true_mu}\")\n",
    "print(f\"  œÉ = {true_sigma}\")\n",
    "print(f\"\\nMAXIMUM LIKELIHOOD ESTIMATES:\")\n",
    "print(f\"  ŒºÃÇ_MLE = {mu_mle:.4f}\")\n",
    "print(f\"  œÉÃÇ_MLE = {sigma_mle:.4f}\")\n",
    "print(f\"\\nüí° MLE Formulas for Normal Distribution:\")\n",
    "print(f\"   ŒºÃÇ_MLE = (1/n)Œ£x·µ¢ = sample mean\")\n",
    "print(f\"   œÉÃÇ_MLE = ‚àö[(1/n)Œ£(x·µ¢-ŒºÃÇ)¬≤] (note: uses n, not n-1!)\")\n",
    "print(f\"\\n‚ö†Ô∏è Note: œÉÃÇ_MLE is slightly biased (underestimates œÉ for small n)\")\n",
    "print(f\"         Use n-1 (ddof=1) for unbiased estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 4: Likelihood function (finding the maximum)\n",
    "\n",
    "# Compute likelihood for different values of Œº (fixing œÉ)\n",
    "mu_values = np.linspace(4.5, 6.0, 100)\n",
    "log_likelihoods = []\n",
    "\n",
    "for mu in mu_values:\n",
    "    # Log-likelihood: sum of log probabilities\n",
    "    log_lik = np.sum(stats.norm.logpdf(data, loc=mu, scale=true_sigma))\n",
    "    log_likelihoods.append(log_lik)\n",
    "\n",
    "log_likelihoods = np.array(log_likelihoods)\n",
    "\n",
    "# Find maximum\n",
    "max_idx = np.argmax(log_likelihoods)\n",
    "mu_at_max = mu_values[max_idx]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot log-likelihood curve\n",
    "plt.plot(mu_values, log_likelihoods, 'b-', linewidth=2, \n",
    "         label='Log-Likelihood ‚Ñì(Œº)')\n",
    "\n",
    "# Mark the maximum\n",
    "plt.scatter([mu_at_max], [log_likelihoods[max_idx]], s=200, c='red', \n",
    "            marker='*', zorder=5, edgecolors='black', linewidths=1.5,\n",
    "            label=f'MLE: ŒºÃÇ = {mu_at_max:.3f}')\n",
    "\n",
    "# Mark true value\n",
    "plt.axvline(true_mu, color='green', linestyle='--', linewidth=2, alpha=0.7,\n",
    "            label=f'True Œº = {true_mu}')\n",
    "\n",
    "# Mark sample mean\n",
    "plt.axvline(mu_mle, color='orange', linestyle=':', linewidth=2, alpha=0.7,\n",
    "            label=f'Sample mean = {mu_mle:.3f}')\n",
    "\n",
    "plt.xlabel('Parameter Œº (mean)', fontsize=12)\n",
    "plt.ylabel('Log-Likelihood ‚Ñì(Œº)', fontsize=12)\n",
    "plt.title('Maximum Likelihood Estimation: Finding Œº that Maximizes Likelihood üéØ', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation\n",
    "plt.annotate('Maximum!', xy=(mu_at_max, log_likelihoods[max_idx]), \n",
    "             xytext=(mu_at_max+0.3, log_likelihoods[max_idx]+5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='red'),\n",
    "             fontsize=12, fontweight='bold', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° MLE Principle:\")\n",
    "print(\"   - Try different parameter values\")\n",
    "print(\"   - For each, calculate: How likely is our data given this parameter?\")\n",
    "print(\"   - Choose parameter that makes data MOST likely (maximum!)\")\n",
    "print(f\"   - For normal distribution, this equals the sample mean!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ MLE Example 2: Exponential Distribution\n",
    "# Time between pest occurrences ~ Exponential(Œª)\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "true_lambda = 0.5  # rate: 0.5 events per day\n",
    "n = 100\n",
    "pest_times = np.random.exponential(scale=1/true_lambda, size=n)\n",
    "\n",
    "# MLE for exponential: ŒªÃÇ = 1/mean(x)\n",
    "lambda_mle = 1 / pest_times.mean()\n",
    "\n",
    "print(\"üêõ MLE for Exponential Distribution (Pest Times):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sample size: n = {n} observations\")\n",
    "print(f\"\\nTRUE PARAMETER:\")\n",
    "print(f\"  Œª = {true_lambda} events/day\")\n",
    "print(f\"  Mean time between events = {1/true_lambda} days\")\n",
    "print(f\"\\nSAMPLE DATA:\")\n",
    "print(f\"  Observed mean time = {pest_times.mean():.3f} days\")\n",
    "print(f\"\\nMAXIMUM LIKELIHOOD ESTIMATE:\")\n",
    "print(f\"  ŒªÃÇ_MLE = {lambda_mle:.4f} events/day\")\n",
    "print(f\"  Estimated mean time = {1/lambda_mle:.3f} days\")\n",
    "print(f\"\\nüí° MLE Formula for Exponential:\")\n",
    "print(f\"   ŒªÃÇ_MLE = 1 / sample_mean\")\n",
    "print(f\"   Simple and intuitive!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 5: Fitted distribution with MLE parameters\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Exponential data with fitted distributions\n",
    "axes[0].hist(pest_times, bins=25, alpha=0.6, color='steelblue', \n",
    "             edgecolor='black', density=True, label='Observed Data')\n",
    "\n",
    "x = np.linspace(0, pest_times.max(), 100)\n",
    "\n",
    "# True distribution\n",
    "axes[0].plot(x, stats.expon.pdf(x, scale=1/true_lambda), 'g--', \n",
    "             linewidth=2, label=f'True (Œª={true_lambda})')\n",
    "\n",
    "# MLE fitted distribution\n",
    "axes[0].plot(x, stats.expon.pdf(x, scale=1/lambda_mle), 'r-', \n",
    "             linewidth=2, label=f'MLE Fit (ŒªÃÇ={lambda_mle:.3f})')\n",
    "\n",
    "axes[0].set_xlabel('Days Between Pest Events', fontsize=11)\n",
    "axes[0].set_ylabel('Density', fontsize=11)\n",
    "axes[0].set_title('MLE Fitted Distribution üéØ', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Likelihood function for Œª\n",
    "lambda_values = np.linspace(0.2, 0.8, 100)\n",
    "log_likelihoods = []\n",
    "\n",
    "for lam in lambda_values:\n",
    "    log_lik = np.sum(stats.expon.logpdf(pest_times, scale=1/lam))\n",
    "    log_likelihoods.append(log_lik)\n",
    "\n",
    "log_likelihoods = np.array(log_likelihoods)\n",
    "\n",
    "axes[1].plot(lambda_values, log_likelihoods, 'b-', linewidth=2)\n",
    "axes[1].scatter([lambda_mle], [log_likelihoods[np.argmax(log_likelihoods)]], \n",
    "                s=200, c='red', marker='*', zorder=5, edgecolors='black', \n",
    "                linewidths=1.5, label=f'MLE: ŒªÃÇ={lambda_mle:.3f}')\n",
    "axes[1].axvline(true_lambda, color='green', linestyle='--', linewidth=2, \n",
    "                alpha=0.7, label=f'True Œª={true_lambda}')\n",
    "axes[1].set_xlabel('Parameter Œª (rate)', fontsize=11)\n",
    "axes[1].set_ylabel('Log-Likelihood', fontsize=11)\n",
    "axes[1].set_title('Likelihood Function', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('MLE for Exponential Distribution üêõ', fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° MLE gives us the 'best fit' distribution for our data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Method of Moments (MoM) üìä\n",
    "\n",
    "### Alternative Estimation Method\n",
    "\n",
    "**Idea**: Match sample moments to population moments\n",
    "\n",
    "### Moments:\n",
    "\n",
    "- **1st moment**: Mean E[X] = Œº\n",
    "- **2nd moment**: E[X¬≤]\n",
    "- **kth moment**: E[X·µè]\n",
    "\n",
    "### Method:\n",
    "\n",
    "1. Express population moments in terms of parameters\n",
    "2. Set sample moments equal to population moments\n",
    "3. Solve for parameters\n",
    "\n",
    "### Example (Normal Distribution):\n",
    "\n",
    "Population moments:\n",
    "- E[X] = Œº\n",
    "- Var[X] = E[X¬≤] - (E[X])¬≤ = œÉ¬≤\n",
    "\n",
    "Sample moments:\n",
    "- (1/n)Œ£x·µ¢\n",
    "- (1/n)Œ£x·µ¢¬≤ - ((1/n)Œ£x·µ¢)¬≤\n",
    "\n",
    "Set equal and solve:\n",
    "- ŒºÃÇ = (1/n)Œ£x·µ¢\n",
    "- œÉÃÇ¬≤ = (1/n)Œ£(x·µ¢ - ŒºÃÇ)¬≤\n",
    "\n",
    "### MoM vs MLE:\n",
    "\n",
    "- **MoM**: Simple, intuitive, easy to compute\n",
    "- **MLE**: More efficient, better properties, but sometimes harder to compute\n",
    "- For many distributions (like normal), they give the same estimates!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Method of Moments vs MLE comparison\n",
    "\n",
    "# Use the same normal data from before\n",
    "print(\"üìä Method of Moments vs Maximum Likelihood:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sample size: n = {n}\")\n",
    "print(f\"\\n1. METHOD OF MOMENTS (MoM):\")\n",
    "print(f\"   ŒºÃÇ_MoM = (1/n)Œ£x·µ¢ = {data.mean():.4f}\")\n",
    "print(f\"   œÉÃÇ¬≤_MoM = (1/n)Œ£(x·µ¢-ŒºÃÇ)¬≤ = {((data - data.mean())**2).mean():.4f}\")\n",
    "\n",
    "print(f\"\\n2. MAXIMUM LIKELIHOOD (MLE):\")\n",
    "print(f\"   ŒºÃÇ_MLE = {mu_mle:.4f}\")\n",
    "print(f\"   œÉÃÇ¬≤_MLE = {sigma_mle**2:.4f}\")\n",
    "\n",
    "print(f\"\\n3. UNBIASED ESTIMATES:\")\n",
    "sigma_unbiased = np.sqrt(data.var(ddof=1))\n",
    "print(f\"   ŒºÃÇ = {data.mean():.4f} (same as MLE and MoM)\")\n",
    "print(f\"   œÉÃÇ¬≤_unbiased = {data.var(ddof=1):.4f} (using n-1)\")\n",
    "\n",
    "print(f\"\\nüí° For Normal Distribution:\")\n",
    "print(f\"   - MoM and MLE give same ŒºÃÇ estimate\")\n",
    "print(f\"   - MoM and MLE give same œÉÃÇ estimate (both use n)\")\n",
    "print(f\"   - For unbiased œÉÃÇ¬≤, use n-1 instead of n\")\n",
    "print(f\"   - MLE is more efficient (lower variance) asymptotically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Machine Learning Connection ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "### Training is Parameter Estimation!\n",
    "\n",
    "When you train an ML model, you're doing **parameter estimation** (usually MLE):\n",
    "\n",
    "#### 1. Linear Regression = MLE\n",
    "\n",
    "**Model**: y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œµ, where Œµ ~ N(0, œÉ¬≤)\n",
    "\n",
    "**MLE**: Minimize sum of squared errors (equivalent to maximizing likelihood!)\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{MLE} = \\arg\\min_{\\beta} \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "#### 2. Logistic Regression = MLE\n",
    "\n",
    "**Model**: P(y=1|x) = œÉ(Œ≤‚ÇÄ + Œ≤‚ÇÅx)\n",
    "\n",
    "**MLE**: Maximize log-likelihood (cross-entropy loss!)\n",
    "\n",
    "$$\n",
    "\\hat{\\beta}_{MLE} = \\arg\\max_{\\beta} \\sum_{i=1}^{n} [y_i \\log p_i + (1-y_i) \\log(1-p_i)]\n",
    "$$\n",
    "\n",
    "#### 3. Neural Networks = MLE\n",
    "\n",
    "**Cross-entropy loss** = Negative log-likelihood\n",
    "\n",
    "**Training** = Finding MLE of network parameters!\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "**Every time you train a model, you're doing MLE!**\n",
    "\n",
    "- Loss function = Negative log-likelihood\n",
    "- Training = Maximizing likelihood (minimizing negative log-likelihood)\n",
    "- Learned parameters = MLE estimates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ ML Example: Logistic Regression as MLE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Generate classification data\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "# Feature: soil nitrogen level\n",
    "X = np.random.normal(7.0, 2.0, n).reshape(-1, 1)\n",
    "\n",
    "# Target: high yield (1) or low yield (0)\n",
    "# Probability depends on nitrogen\n",
    "prob_high_yield = 1 / (1 + np.exp(-0.8 * (X.ravel() - 7.0)))\n",
    "y = (np.random.random(n) < prob_high_yield).astype(int)\n",
    "\n",
    "# Train logistic regression (MLE!)\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get MLE parameters\n",
    "beta0_mle = model.intercept_[0]\n",
    "beta1_mle = model.coef_[0][0]\n",
    "\n",
    "# Calculate log-likelihood\n",
    "y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "log_likelihood = -log_loss(y, y_pred_proba, normalize=False)\n",
    "\n",
    "print(\"ü§ñ Logistic Regression as MLE:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sample size: n = {n}\")\n",
    "print(f\"\\nPROBLEM: Predict high yield based on soil nitrogen\")\n",
    "print(f\"\\nLOGISTIC MODEL:\")\n",
    "print(f\"  P(high yield | nitrogen) = œÉ(Œ≤‚ÇÄ + Œ≤‚ÇÅ √ó nitrogen)\")\n",
    "print(f\"\\nMLE ESTIMATES (from sklearn):\")\n",
    "print(f\"  Œ≤ÃÇ‚ÇÄ = {beta0_mle:.4f}\")\n",
    "print(f\"  Œ≤ÃÇ‚ÇÅ = {beta1_mle:.4f}\")\n",
    "print(f\"\\nLog-Likelihood at MLE: {log_likelihood:.2f}\")\n",
    "print(f\"\\nüí° Key Insight:\")\n",
    "print(f\"   - sklearn.fit() finds parameters that MAXIMIZE likelihood\")\n",
    "print(f\"   - This is exactly MLE!\")\n",
    "print(f\"   - Cross-entropy loss = Negative log-likelihood\")\n",
    "print(f\"   - Training = MLE parameter estimation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 6: Likelihood surface for logistic regression\n",
    "\n",
    "# Create grid of parameter values\n",
    "beta0_range = np.linspace(beta0_mle - 2, beta0_mle + 2, 50)\n",
    "beta1_range = np.linspace(beta1_mle - 1, beta1_mle + 1, 50)\n",
    "Beta0, Beta1 = np.meshgrid(beta0_range, beta1_range)\n",
    "\n",
    "# Calculate log-likelihood for each combination\n",
    "LogLik = np.zeros_like(Beta0)\n",
    "\n",
    "for i in range(Beta0.shape[0]):\n",
    "    for j in range(Beta0.shape[1]):\n",
    "        b0, b1 = Beta0[i, j], Beta1[i, j]\n",
    "        # Calculate predicted probabilities\n",
    "        logits = b0 + b1 * X.ravel()\n",
    "        probs = 1 / (1 + np.exp(-logits))\n",
    "        # Log-likelihood\n",
    "        log_lik = np.sum(y * np.log(probs + 1e-10) + (1 - y) * np.log(1 - probs + 1e-10))\n",
    "        LogLik[i, j] = log_lik\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Contour plot\n",
    "contour = plt.contourf(Beta0, Beta1, LogLik, levels=20, cmap='viridis', alpha=0.8)\n",
    "plt.colorbar(contour, label='Log-Likelihood')\n",
    "\n",
    "# Mark the MLE\n",
    "plt.scatter([beta0_mle], [beta1_mle], s=300, c='red', marker='*', \n",
    "            edgecolors='white', linewidths=2, zorder=5,\n",
    "            label=f'MLE: (Œ≤ÃÇ‚ÇÄ={beta0_mle:.2f}, Œ≤ÃÇ‚ÇÅ={beta1_mle:.2f})')\n",
    "\n",
    "plt.xlabel('Intercept Œ≤‚ÇÄ', fontsize=12)\n",
    "plt.ylabel('Coefficient Œ≤‚ÇÅ', fontsize=12)\n",
    "plt.title('Likelihood Surface: Training Finds the Peak (MLE)! üèîÔ∏è', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11, loc='upper left')\n",
    "plt.grid(True, alpha=0.3, color='white')\n",
    "\n",
    "# Add annotation\n",
    "plt.annotate('Maximum\\nLikelihood!', xy=(beta0_mle, beta1_mle), \n",
    "             xytext=(beta0_mle-1, beta1_mle+0.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=2, color='white'),\n",
    "             fontsize=12, fontweight='bold', color='white',\n",
    "             bbox=dict(boxstyle='round', facecolor='red', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° ML Training Visualized:\")\n",
    "print(\"   - Each point (Œ≤‚ÇÄ, Œ≤‚ÇÅ) gives a different model\")\n",
    "print(\"   - Color shows log-likelihood (brighter = more likely)\")\n",
    "print(\"   - Training algorithm searches this space\")\n",
    "print(\"   - Finds peak (maximum likelihood) = MLE\")\n",
    "print(\"   - This is what model.fit() does!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways üéØ\n",
    "\n",
    "### Point Estimation:\n",
    "\n",
    "1. ‚úÖ **Estimator vs Estimate**:\n",
    "   - Estimator: Rule/formula (random variable)\n",
    "   - Estimate: Specific numerical result (fixed number)\n",
    "   - Parameter: True population value (unknown)\n",
    "\n",
    "2. ‚úÖ **Good Estimator Properties**:\n",
    "   - **Unbiased**: E[Œ∏ÃÇ] = Œ∏ (correct on average)\n",
    "   - **Consistent**: Œ∏ÃÇ ‚Üí Œ∏ as n ‚Üí ‚àû (converges to truth)\n",
    "   - **Efficient**: Lowest variance among unbiased estimators\n",
    "\n",
    "3. ‚úÖ **Maximum Likelihood Estimation (MLE)** ‚≠ê‚≠ê:\n",
    "   - Choose Œ∏ that makes data most likely\n",
    "   - Maximize L(Œ∏|x) or log-likelihood ‚Ñì(Œ∏|x)\n",
    "   - Gold standard: consistent, efficient, asymptotically normal\n",
    "\n",
    "4. ‚úÖ **Method of Moments (MoM)**:\n",
    "   - Match sample moments to population moments\n",
    "   - Simple and intuitive\n",
    "   - Often gives same result as MLE\n",
    "\n",
    "5. ‚úÖ **ML Connection** ‚≠ê‚≠ê‚≠ê:\n",
    "   - **Training is parameter estimation!**\n",
    "   - Most ML training = MLE\n",
    "   - Loss functions = Negative log-likelihood\n",
    "   - Linear regression = MLE with normal errors\n",
    "   - Logistic regression = MLE with Bernoulli\n",
    "   - Neural networks = MLE\n",
    "\n",
    "### Critical Formula:\n",
    "\n",
    "$$\n",
    "\\boxed{\\hat{\\theta}_{MLE} = \\arg\\max_{\\theta} L(\\theta | x) = \\arg\\max_{\\theta} \\sum_{i=1}^{n} \\ln f(x_i | \\theta)}\n",
    "$$\n",
    "\n",
    "**This is the foundation of machine learning training!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps üöÄ\n",
    "\n",
    "**Coming Up Next: Confidence Intervals** ‚≠ê‚≠ê\n",
    "\n",
    "We've learned to make **point estimates**, but every estimate has uncertainty!\n",
    "\n",
    "**Question**: If ŒºÃÇ = 5.15, how confident are we? Is the true Œº between 5.0 and 5.3?\n",
    "\n",
    "In the next notebook, we'll learn:\n",
    "- **Confidence Intervals**: Quantify uncertainty in estimates\n",
    "- **Correct interpretation**: What does \"95% confidence\" really mean?\n",
    "- **Construction**: Using t-distribution for unknown œÉ\n",
    "- **ML Application**: Reporting model performance with uncertainty ‚≠ê\n",
    "\n",
    "**Example**: \n",
    "- Point estimate: \"Model accuracy = 0.85\"\n",
    "- With CI: \"Model accuracy = 0.85 ¬± 0.03 (95% CI: [0.82, 0.88])\"\n",
    "\n",
    "The second statement is much more informative!\n",
    "\n",
    "See you in **`04_confidence_intervals.ipynb`**!\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work! You now understand the foundations of parameter estimation and how ML training works!** üéØ‚ú®üåæ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
