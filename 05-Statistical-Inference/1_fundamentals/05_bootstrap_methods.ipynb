{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap Methods: Modern Computer-Intensive Inference üéí\n",
    "\n",
    "## Introduction: Inference Without Formulas!\n",
    "\n",
    "In previous notebooks, we learned classical inference methods:\n",
    "- **Point estimation**: Calculate xÃÑ, s¬≤\n",
    "- **Confidence intervals**: xÃÑ ¬± t* √ó (s/‚àön)\n",
    "- **Requirements**: Know formulas, make distributional assumptions\n",
    "\n",
    "**But what if**:\n",
    "- You want CI for median, correlation, or some complex statistic?\n",
    "- No formula exists for the SE?\n",
    "- Distributional assumptions don't hold?\n",
    "- You want a modern, flexible approach?\n",
    "\n",
    "### The Solution: Bootstrap! üéØ\n",
    "\n",
    "**Key Idea**: Use the sample itself to estimate sampling variability\n",
    "\n",
    "1. Treat your sample as a \"mini-population\"\n",
    "2. Resample from it WITH replacement (many times)\n",
    "3. Calculate statistic for each resample\n",
    "4. Use distribution of bootstrap statistics for inference\n",
    "\n",
    "**No formulas needed! No distributional assumptions!**\n",
    "\n",
    "### ML Connection ü§ñ\n",
    "\n",
    "Bootstrap is the foundation of many modern ML techniques:\n",
    "\n",
    "- **Bootstrap Aggregating (Bagging)**: Train models on bootstrap samples, average predictions\n",
    "- **Random Forests** = Bootstrap + Decision Trees ‚≠ê‚≠ê\n",
    "- **Bootstrap evaluation**: Robust model performance estimates\n",
    "- **Feature importance**: Quantify uncertainty in importance scores\n",
    "\n",
    "Understanding bootstrap helps you understand why ensemble methods work!\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. ‚úÖ Understand **bootstrap resampling** philosophy ‚≠ê\n",
    "2. ‚úÖ Implement bootstrap from scratch for any statistic\n",
    "3. ‚úÖ Calculate **bootstrap confidence intervals**\n",
    "4. ‚úÖ Apply bootstrap to complex statistics (correlation, difference, etc.)\n",
    "5. ‚úÖ Connect to ML: **Bagging and Random Forests** ‚≠ê‚≠ê‚≠ê\n",
    "6. ‚úÖ Understand when bootstrap works and when to be cautious\n",
    "\n",
    "‚≠ê‚≠ê‚≠ê = Most critical ML connection\n",
    "\n",
    "---\n",
    "\n",
    "Let's bootstrap! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Setup: Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Set style for beautiful plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Setup complete!\")\n",
    "print(\"üéí Ready to learn bootstrap methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Bootstrap Philosophy ‚≠ê\n",
    "\n",
    "### The Core Idea:\n",
    "\n",
    "**Problem**: We have ONE sample, but we want to know about the sampling distribution\n",
    "\n",
    "**Classical Approach**: Use formulas and assumptions (CLT, normal distribution, etc.)\n",
    "\n",
    "**Bootstrap Approach**: \n",
    "1. Treat sample as if it were the population\n",
    "2. Draw many new samples FROM this sample (with replacement!)\n",
    "3. Calculate statistic for each new sample\n",
    "4. The distribution of these statistics approximates the sampling distribution\n",
    "\n",
    "### Why \"With Replacement\"?\n",
    "\n",
    "- We need to mimic the original sampling process\n",
    "- Original sample: drew n observations from population (‚àû size)\n",
    "- Bootstrap sample: draw n observations from sample (finite size)\n",
    "- **With replacement** allows us to get variability!\n",
    "\n",
    "### Example:\n",
    "\n",
    "Original sample: [5.1, 5.3, 4.9, 5.2, 5.4]\n",
    "\n",
    "Bootstrap samples (with replacement):\n",
    "- [5.1, 5.3, 5.3, 5.2, 4.9] (5.3 appears twice!)\n",
    "- [5.4, 5.1, 5.1, 5.1, 5.2] (5.1 appears three times!)\n",
    "- [4.9, 5.2, 5.4, 5.3, 5.2] (different order, 5.2 twice)\n",
    "\n",
    "Calculate mean for each ‚Üí bootstrap sampling distribution!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåæ Simple bootstrap example\n",
    "\n",
    "# Original sample: wheat yields from 20 fields\n",
    "np.random.seed(42)\n",
    "original_sample = np.array([5.1, 5.3, 4.9, 5.2, 5.4, 5.0, 5.2, 4.8, 5.3, 5.1,\n",
    "                            5.2, 5.4, 5.0, 4.9, 5.3, 5.1, 5.2, 5.3, 5.0, 5.1])\n",
    "\n",
    "n = len(original_sample)\n",
    "\n",
    "# Take a few bootstrap samples\n",
    "print(\"üéí Bootstrap Resampling Example:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original sample (n={n}):\")\n",
    "print(f\"  {original_sample}\")\n",
    "print(f\"  Mean: {original_sample.mean():.3f}\")\n",
    "print(f\"\\nBootstrap samples (each with n={n}, sampled WITH replacement):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i in range(5):\n",
    "    # Resample with replacement\n",
    "    bootstrap_sample = np.random.choice(original_sample, size=n, replace=True)\n",
    "    \n",
    "    # Show unique values to demonstrate repetition\n",
    "    unique, counts = np.unique(bootstrap_sample, return_counts=True)\n",
    "    \n",
    "    print(f\"\\nBootstrap sample {i+1}:\")\n",
    "    print(f\"  {bootstrap_sample.round(1)}\")\n",
    "    print(f\"  Mean: {bootstrap_sample.mean():.3f}\")\n",
    "    \n",
    "    # Show which values were repeated\n",
    "    repeated = unique[counts > 1]\n",
    "    if len(repeated) > 0:\n",
    "        print(f\"  Repeated values: {repeated} (that's OK! That's the point!)\")\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   - Each bootstrap sample has same size as original (n=20)\")\n",
    "print(\"   - Values can repeat (sampled WITH replacement)\")\n",
    "print(\"   - Each bootstrap sample gives different mean\")\n",
    "print(\"   - This variability mimics sampling variability!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 1: Original vs bootstrap sample\n",
    "\n",
    "# Take one bootstrap sample\n",
    "np.random.seed(42)\n",
    "bootstrap_sample = np.random.choice(original_sample, size=n, replace=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Original sample\n",
    "unique_orig, counts_orig = np.unique(original_sample, return_counts=True)\n",
    "axes[0].bar(unique_orig, counts_orig, width=0.05, alpha=0.7, \n",
    "            color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Yield (tons/hectare)', fontsize=11)\n",
    "axes[0].set_ylabel('Frequency', fontsize=11)\n",
    "axes[0].set_title('Original Sample (n=20)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].axvline(original_sample.mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Mean = {original_sample.mean():.2f}')\n",
    "axes[0].legend(fontsize=10)\n",
    "\n",
    "# Right: Bootstrap sample\n",
    "unique_boot, counts_boot = np.unique(bootstrap_sample, return_counts=True)\n",
    "axes[1].bar(unique_boot, counts_boot, width=0.05, alpha=0.7, \n",
    "            color='orange', edgecolor='black')\n",
    "axes[1].set_xlabel('Yield (tons/hectare)', fontsize=11)\n",
    "axes[1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1].set_title('Bootstrap Sample (n=20, with replacement)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].axvline(bootstrap_sample.mean(), color='red', linestyle='--', \n",
    "                linewidth=2, label=f'Mean = {bootstrap_sample.mean():.2f}')\n",
    "axes[1].legend(fontsize=10)\n",
    "\n",
    "# Highlight repeated values\n",
    "for val, count in zip(unique_boot, counts_boot):\n",
    "    if count > 1:\n",
    "        axes[1].text(val, count, f'{count}√ó', ha='center', va='bottom', \n",
    "                    fontsize=9, fontweight='bold', color='red')\n",
    "\n",
    "plt.suptitle('Bootstrap Resampling: WITH Replacement üéí', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice:\")\n",
    "print(\"   - Some values appear more than once in bootstrap sample (red numbers)\")\n",
    "print(\"   - Some original values might not appear at all\")\n",
    "print(\"   - This creates variability ‚Üí mimics sampling from population!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Bootstrap Algorithm üîß\n",
    "\n",
    "### Step-by-Step Procedure:\n",
    "\n",
    "**Input**: \n",
    "- Original sample: x‚ÇÅ, x‚ÇÇ, ..., x‚Çô\n",
    "- Statistic of interest: Œ∏ (mean, median, correlation, etc.)\n",
    "- Number of bootstrap samples: B (typically 1000-10000)\n",
    "\n",
    "**Algorithm**:\n",
    "```\n",
    "FOR b = 1 to B:\n",
    "    1. Draw n observations from original sample WITH replacement\n",
    "       ‚Üí bootstrap sample: x*‚ÇÅ, x*‚ÇÇ, ..., x*‚Çô\n",
    "    \n",
    "    2. Calculate statistic on bootstrap sample\n",
    "       ‚Üí Œ∏*·µ¶ = f(x*‚ÇÅ, x*‚ÇÇ, ..., x*‚Çô)\n",
    "    \n",
    "    3. Store Œ∏*·µ¶\n",
    "\n",
    "OUTPUT: Bootstrap distribution {Œ∏*‚ÇÅ, Œ∏*‚ÇÇ, ..., Œ∏*·µ¶}\n",
    "```\n",
    "\n",
    "### Uses:\n",
    "\n",
    "1. **Bootstrap SE**: SE = standard deviation of {Œ∏*‚ÇÅ, ..., Œ∏*·µ¶}\n",
    "2. **Bootstrap CI**: Use percentiles of bootstrap distribution\n",
    "3. **Bias estimation**: Bias = mean(Œ∏*) - Œ∏ÃÇ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Implement bootstrap from scratch\n",
    "\n",
    "def bootstrap(data, statistic_func, n_bootstrap=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Perform bootstrap resampling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Original sample data\n",
    "    statistic_func : function\n",
    "        Function to calculate statistic (e.g., np.mean, np.median)\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap samples\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bootstrap_statistics : array\n",
    "        Bootstrap distribution of the statistic\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n = len(data)\n",
    "    bootstrap_statistics = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        bootstrap_sample = np.random.choice(data, size=n, replace=True)\n",
    "        \n",
    "        # Calculate statistic\n",
    "        stat = statistic_func(bootstrap_sample)\n",
    "        bootstrap_statistics.append(stat)\n",
    "    \n",
    "    return np.array(bootstrap_statistics)\n",
    "\n",
    "print(\"üîß Bootstrap Implementation:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Function: bootstrap(data, statistic_func, n_bootstrap=1000)\")\n",
    "print(\"\\nAlgorithm:\")\n",
    "print(\"  1. For each of B bootstrap iterations:\")\n",
    "print(\"     a. Resample n observations WITH replacement\")\n",
    "print(\"     b. Calculate statistic on bootstrap sample\")\n",
    "print(\"     c. Store result\")\n",
    "print(\"  2. Return array of B bootstrap statistics\")\n",
    "print(\"\\n‚úì Bootstrap function ready to use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Apply bootstrap to estimate sampling distribution of mean\n",
    "\n",
    "# Bootstrap for the mean\n",
    "B = 1000\n",
    "bootstrap_means = bootstrap(original_sample, np.mean, n_bootstrap=B)\n",
    "\n",
    "# Calculate statistics\n",
    "original_mean = original_sample.mean()\n",
    "bootstrap_mean = bootstrap_means.mean()\n",
    "bootstrap_se = bootstrap_means.std()\n",
    "\n",
    "# Compare with theoretical SE\n",
    "theoretical_se = original_sample.std(ddof=1) / np.sqrt(len(original_sample))\n",
    "\n",
    "print(\"üéØ Bootstrap Distribution of the Mean:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original sample size: n = {len(original_sample)}\")\n",
    "print(f\"Number of bootstrap samples: B = {B}\")\n",
    "print(f\"\\nORIGINAL SAMPLE:\")\n",
    "print(f\"  Sample mean: {original_mean:.4f}\")\n",
    "print(f\"\\nBOOTSTRAP DISTRIBUTION:\")\n",
    "print(f\"  Mean of bootstrap means: {bootstrap_mean:.4f}\")\n",
    "print(f\"  Bootstrap SE: {bootstrap_se:.4f}\")\n",
    "print(f\"\\nCOMPARISON:\")\n",
    "print(f\"  Theoretical SE = s/‚àön: {theoretical_se:.4f}\")\n",
    "print(f\"  Bootstrap SE: {bootstrap_se:.4f}\")\n",
    "print(f\"  Difference: {abs(bootstrap_se - theoretical_se):.4f}\")\n",
    "print(f\"\\nüí° Bootstrap SE is very close to theoretical SE!\")\n",
    "print(f\"   And we didn't need any formulas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 2: Bootstrap distribution of the mean\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram of bootstrap means\n",
    "plt.hist(bootstrap_means, bins=40, alpha=0.7, color='steelblue', \n",
    "         edgecolor='black', density=True, label=f'{B} Bootstrap Means')\n",
    "\n",
    "# Overlay normal distribution (theoretical)\n",
    "x = np.linspace(bootstrap_means.min(), bootstrap_means.max(), 100)\n",
    "plt.plot(x, stats.norm.pdf(x, original_mean, theoretical_se), 'r-', \n",
    "         linewidth=2, label=f'Theoretical: N({original_mean:.2f}, {theoretical_se:.3f})')\n",
    "\n",
    "# Mark original sample mean\n",
    "plt.axvline(original_mean, color='black', linestyle='--', linewidth=2, \n",
    "            label=f'Original mean = {original_mean:.3f}')\n",
    "\n",
    "# Mark bootstrap mean\n",
    "plt.axvline(bootstrap_mean, color='green', linestyle=':', linewidth=2, \n",
    "            alpha=0.7, label=f'Bootstrap mean = {bootstrap_mean:.3f}')\n",
    "\n",
    "plt.xlabel('Sample Mean (tons/hectare)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title(f'Bootstrap Sampling Distribution (B={B}) üéí', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add text box\n",
    "textstr = f'Bootstrap SE: {bootstrap_se:.4f}\\nTheoretical SE: {theoretical_se:.4f}'\n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=11,\n",
    "         verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Bootstrap Magic:\")\n",
    "print(\"   - From ONE sample, we created 1000 'pseudo-samples'\")\n",
    "print(\"   - Distribution approximates the true sampling distribution\")\n",
    "print(\"   - No formulas, no assumptions needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Bootstrap Standard Error ‚≠ê\n",
    "\n",
    "### The Power: Works for ANY Statistic!\n",
    "\n",
    "**Classical approach**: Need formula for SE\n",
    "- SE for mean: œÉ/‚àön ‚úì\n",
    "- SE for median: ??? (complicated!)\n",
    "- SE for correlation: ??? (very complicated!)\n",
    "- SE for custom statistic: ??? (might not exist!)\n",
    "\n",
    "**Bootstrap approach**: Same algorithm for everything!\n",
    "\n",
    "$$\n",
    "SE_{bootstrap} = \\text{Standard Deviation of Bootstrap Statistics}\n",
    "$$\n",
    "\n",
    "### Procedure:\n",
    "\n",
    "1. Calculate bootstrap distribution\n",
    "2. SE = standard deviation of bootstrap distribution\n",
    "3. Done! (No formula needed)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ Bootstrap SE for multiple statistics\n",
    "\n",
    "B = 2000\n",
    "\n",
    "# Different statistics\n",
    "statistics = {\n",
    "    'Mean': np.mean,\n",
    "    'Median': np.median,\n",
    "    'Std Dev': lambda x: np.std(x, ddof=1),\n",
    "    '75th Percentile': lambda x: np.percentile(x, 75),\n",
    "    'Coef of Variation': lambda x: np.std(x, ddof=1) / np.mean(x)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"üî¨ Bootstrap Standard Error for Various Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Sample size: n = {len(original_sample)}\")\n",
    "print(f\"Bootstrap samples: B = {B}\")\n",
    "print(f\"\\n{'Statistic':<20} {'Estimate':<12} {'Bootstrap SE':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, func in statistics.items():\n",
    "    # Original estimate\n",
    "    original_stat = func(original_sample)\n",
    "    \n",
    "    # Bootstrap distribution\n",
    "    bootstrap_dist = bootstrap(original_sample, func, n_bootstrap=B)\n",
    "    \n",
    "    # Bootstrap SE\n",
    "    boot_se = bootstrap_dist.std()\n",
    "    \n",
    "    results[name] = {\n",
    "        'estimate': original_stat,\n",
    "        'bootstrap_se': boot_se,\n",
    "        'distribution': bootstrap_dist\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:<20} {original_stat:<12.4f} {boot_se:<15.4f}\")\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   - Same bootstrap algorithm works for ALL statistics!\")\n",
    "print(\"   - No formulas needed\")\n",
    "print(\"   - Works even when no theoretical formula exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 3: Bootstrap distributions for multiple statistics\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "stats_to_plot = ['Mean', 'Median', '75th Percentile', 'Coef of Variation']\n",
    "colors = ['steelblue', 'orange', 'green', 'purple']\n",
    "\n",
    "for idx, (stat_name, color) in enumerate(zip(stats_to_plot, colors)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    dist = results[stat_name]['distribution']\n",
    "    estimate = results[stat_name]['estimate']\n",
    "    se = results[stat_name]['bootstrap_se']\n",
    "    \n",
    "    # Histogram\n",
    "    ax.hist(dist, bins=30, alpha=0.7, color=color, edgecolor='black', density=True)\n",
    "    \n",
    "    # Mark original estimate\n",
    "    ax.axvline(estimate, color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Estimate = {estimate:.3f}')\n",
    "    \n",
    "    # Mark ¬±1 SE\n",
    "    ax.axvline(estimate - se, color='green', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "    ax.axvline(estimate + se, color='green', linestyle=':', linewidth=1.5, alpha=0.7,\n",
    "               label=f'¬±1 SE')\n",
    "    \n",
    "    ax.set_xlabel(f'{stat_name}', fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "    ax.set_title(f'{stat_name}\\n(SE = {se:.4f})', fontsize=11, fontweight='bold')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Bootstrap Distributions for Multiple Statistics üìä', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Flexibility of Bootstrap:\")\n",
    "print(\"   - Works for symmetric statistics (mean)\")\n",
    "print(\"   - Works for skewed statistics (median, percentiles)\")\n",
    "print(\"   - Works for complex statistics (coefficient of variation)\")\n",
    "print(\"   - All with the SAME algorithm!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Bootstrap Confidence Intervals üìä\n",
    "\n",
    "### Percentile Method (Most Common)\n",
    "\n",
    "**Idea**: Use the percentiles of the bootstrap distribution\n",
    "\n",
    "**95% CI**:\n",
    "$$\n",
    "[\\text{2.5th percentile}, \\text{97.5th percentile}]\n",
    "$$\n",
    "\n",
    "**Algorithm**:\n",
    "1. Generate bootstrap distribution {Œ∏*‚ÇÅ, Œ∏*‚ÇÇ, ..., Œ∏*·µ¶}\n",
    "2. Sort the bootstrap statistics\n",
    "3. For 95% CI:\n",
    "   - Lower bound = 2.5th percentile\n",
    "   - Upper bound = 97.5th percentile\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- ‚úÖ Works for ANY statistic\n",
    "- ‚úÖ No distributional assumptions\n",
    "- ‚úÖ Handles skewed distributions naturally\n",
    "- ‚úÖ Simple to implement\n",
    "\n",
    "### Example:\n",
    "\n",
    "For B=1000 bootstrap samples, 95% CI uses the 25th and 975th ordered values\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Calculate bootstrap confidence intervals\n",
    "\n",
    "def bootstrap_ci(data, statistic_func, n_bootstrap=2000, confidence=0.95, seed=42):\n",
    "    \"\"\"\n",
    "    Calculate bootstrap confidence interval using percentile method.\n",
    "    \"\"\"\n",
    "    # Generate bootstrap distribution\n",
    "    boot_dist = bootstrap(data, statistic_func, n_bootstrap, seed)\n",
    "    \n",
    "    # Calculate percentiles\n",
    "    alpha = 1 - confidence\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "    \n",
    "    ci_lower = np.percentile(boot_dist, lower_percentile)\n",
    "    ci_upper = np.percentile(boot_dist, upper_percentile)\n",
    "    \n",
    "    return ci_lower, ci_upper, boot_dist\n",
    "\n",
    "# Calculate 95% CI for mean\n",
    "ci_lower, ci_upper, boot_dist = bootstrap_ci(original_sample, np.mean)\n",
    "\n",
    "print(\"üìä Bootstrap Confidence Interval (Percentile Method):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Statistic: Sample mean\")\n",
    "print(f\"Bootstrap samples: B = {len(boot_dist)}\")\n",
    "print(f\"Confidence level: 95%\")\n",
    "print(f\"\\nPOINT ESTIMATE:\")\n",
    "print(f\"  Sample mean: {original_sample.mean():.4f}\")\n",
    "print(f\"\\nBOOTSTRAP 95% CI:\")\n",
    "print(f\"  Lower bound (2.5th percentile): {ci_lower:.4f}\")\n",
    "print(f\"  Upper bound (97.5th percentile): {ci_upper:.4f}\")\n",
    "print(f\"  CI: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "print(f\"  Width: {ci_upper - ci_lower:.4f}\")\n",
    "\n",
    "# Compare with classical t-based CI\n",
    "n = len(original_sample)\n",
    "s = original_sample.std(ddof=1)\n",
    "t_star = stats.t.ppf(0.975, n-1)\n",
    "classical_ci = (original_sample.mean() - t_star * s / np.sqrt(n),\n",
    "                original_sample.mean() + t_star * s / np.sqrt(n))\n",
    "\n",
    "print(f\"\\nCOMPARISON WITH CLASSICAL CI:\")\n",
    "print(f\"  Classical t-CI: [{classical_ci[0]:.4f}, {classical_ci[1]:.4f}]\")\n",
    "print(f\"  Bootstrap CI:   [{ci_lower:.4f}, {ci_upper:.4f}]\")\n",
    "print(f\"\\nüí° Bootstrap CI is similar to classical CI for the mean\")\n",
    "print(f\"   But bootstrap works for ANY statistic!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 4: Bootstrap distribution with CI\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.hist(boot_dist, bins=40, alpha=0.7, color='steelblue', \n",
    "         edgecolor='black', density=True, label='Bootstrap Distribution')\n",
    "\n",
    "# Shade the 95% CI region\n",
    "x_fill = boot_dist[(boot_dist >= ci_lower) & (boot_dist <= ci_upper)]\n",
    "plt.hist(x_fill, bins=40, alpha=0.5, color='green', \n",
    "         edgecolor='none', density=True, label='95% CI Region')\n",
    "\n",
    "# Mark the bounds\n",
    "plt.axvline(ci_lower, color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Lower: {ci_lower:.3f}')\n",
    "plt.axvline(ci_upper, color='green', linestyle='--', linewidth=2, \n",
    "            label=f'Upper: {ci_upper:.3f}')\n",
    "\n",
    "# Mark original estimate\n",
    "plt.axvline(original_sample.mean(), color='red', linestyle='-', linewidth=2,\n",
    "            label=f'Estimate: {original_sample.mean():.3f}')\n",
    "\n",
    "plt.xlabel('Sample Mean (tons/hectare)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('Bootstrap Distribution with 95% Confidence Interval üìä', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10, loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate('2.5% of\\nbootstrap\\nsamples', xy=(ci_lower - 0.03, 1.5), \n",
    "             xytext=(ci_lower - 0.15, 2.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=1.5),\n",
    "             fontsize=9, ha='center')\n",
    "\n",
    "plt.annotate('2.5% of\\nbootstrap\\nsamples', xy=(ci_upper + 0.03, 1.5), \n",
    "             xytext=(ci_upper + 0.15, 2.5),\n",
    "             arrowprops=dict(arrowstyle='->', lw=1.5),\n",
    "             fontsize=9, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Percentile Method:\")\n",
    "print(\"   - Green region contains middle 95% of bootstrap statistics\")\n",
    "print(\"   - This is our 95% confidence interval\")\n",
    "print(\"   - No assumptions about normality needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Bootstrap for Complex Statistics ‚≠ê\n",
    "\n",
    "### The Real Power of Bootstrap\n",
    "\n",
    "Bootstrap shines when dealing with complex statistics where:\n",
    "- No formula exists for SE\n",
    "- Distribution is unknown or complicated\n",
    "- Classical methods don't apply\n",
    "\n",
    "### Examples:\n",
    "\n",
    "1. **Correlation coefficient**:\n",
    "   - Classical SE formula is complicated\n",
    "   - Bootstrap: Just calculate correlation for each bootstrap sample!\n",
    "\n",
    "2. **Difference in means** (two groups):\n",
    "   - Classical: t-test formula\n",
    "   - Bootstrap: Resample each group, calculate difference\n",
    "\n",
    "3. **Custom metrics**:\n",
    "   - Your own metric for which no theory exists\n",
    "   - Bootstrap gives you SE and CI automatically!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ Bootstrap CI for correlation\n",
    "\n",
    "# Generate correlated data: soil nitrogen vs wheat yield\n",
    "np.random.seed(42)\n",
    "n = 50\n",
    "soil_nitrogen = np.random.normal(7.0, 1.5, n)\n",
    "wheat_yield = 3.0 + 0.3 * soil_nitrogen + np.random.normal(0, 0.5, n)\n",
    "\n",
    "# Combine into pairs\n",
    "data_pairs = np.column_stack([soil_nitrogen, wheat_yield])\n",
    "\n",
    "# Define correlation function\n",
    "def correlation(data):\n",
    "    return np.corrcoef(data[:, 0], data[:, 1])[0, 1]\n",
    "\n",
    "# Original correlation\n",
    "original_corr = correlation(data_pairs)\n",
    "\n",
    "# Bootstrap for correlation\n",
    "B = 2000\n",
    "bootstrap_corrs = []\n",
    "\n",
    "for _ in range(B):\n",
    "    # Resample PAIRS (important!)\n",
    "    indices = np.random.choice(n, size=n, replace=True)\n",
    "    bootstrap_sample = data_pairs[indices]\n",
    "    bootstrap_corrs.append(correlation(bootstrap_sample))\n",
    "\n",
    "bootstrap_corrs = np.array(bootstrap_corrs)\n",
    "\n",
    "# Calculate CI\n",
    "corr_ci_lower = np.percentile(bootstrap_corrs, 2.5)\n",
    "corr_ci_upper = np.percentile(bootstrap_corrs, 97.5)\n",
    "corr_se = bootstrap_corrs.std()\n",
    "\n",
    "print(\"üî¨ Bootstrap CI for Correlation Coefficient:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Variables: Soil nitrogen vs Wheat yield\")\n",
    "print(f\"Sample size: n = {n} fields\")\n",
    "print(f\"Bootstrap samples: B = {B}\")\n",
    "print(f\"\\nRESULTS:\")\n",
    "print(f\"  Sample correlation: r = {original_corr:.4f}\")\n",
    "print(f\"  Bootstrap SE: {corr_se:.4f}\")\n",
    "print(f\"  95% Bootstrap CI: [{corr_ci_lower:.4f}, {corr_ci_upper:.4f}]\")\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"   We are 95% confident the true correlation between\")\n",
    "print(f\"   soil nitrogen and wheat yield is between\")\n",
    "print(f\"   {corr_ci_lower:.2f} and {corr_ci_upper:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 5: Scatter plot with bootstrap correlation distribution\n",
    "\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Main scatter plot\n",
    "ax_main = plt.subplot(1, 2, 1)\n",
    "ax_main.scatter(soil_nitrogen, wheat_yield, alpha=0.6, s=80, \n",
    "                edgecolors='black', linewidths=0.5)\n",
    "\n",
    "# Add regression line\n",
    "z = np.polyfit(soil_nitrogen, wheat_yield, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(soil_nitrogen.min(), soil_nitrogen.max(), 100)\n",
    "ax_main.plot(x_line, p(x_line), 'r-', linewidth=2, alpha=0.7)\n",
    "\n",
    "ax_main.set_xlabel('Soil Nitrogen (kg/ha)', fontsize=11)\n",
    "ax_main.set_ylabel('Wheat Yield (tons/hectare)', fontsize=11)\n",
    "ax_main.set_title(f'Soil Nitrogen vs Yield\\n(r = {original_corr:.3f})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "ax_main.grid(True, alpha=0.3)\n",
    "\n",
    "# Bootstrap distribution\n",
    "ax_boot = plt.subplot(1, 2, 2)\n",
    "ax_boot.hist(bootstrap_corrs, bins=40, alpha=0.7, color='steelblue', \n",
    "             edgecolor='black', density=True)\n",
    "\n",
    "# Shade CI\n",
    "x_fill = bootstrap_corrs[(bootstrap_corrs >= corr_ci_lower) & \n",
    "                         (bootstrap_corrs <= corr_ci_upper)]\n",
    "ax_boot.hist(x_fill, bins=40, alpha=0.5, color='green', \n",
    "             edgecolor='none', density=True)\n",
    "\n",
    "ax_boot.axvline(original_corr, color='red', linestyle='--', linewidth=2,\n",
    "                label=f'r = {original_corr:.3f}')\n",
    "ax_boot.axvline(corr_ci_lower, color='green', linestyle=':', linewidth=2,\n",
    "                label=f'95% CI: [{corr_ci_lower:.3f}, {corr_ci_upper:.3f}]')\n",
    "ax_boot.axvline(corr_ci_upper, color='green', linestyle=':', linewidth=2)\n",
    "\n",
    "ax_boot.set_xlabel('Correlation Coefficient', fontsize=11)\n",
    "ax_boot.set_ylabel('Density', fontsize=11)\n",
    "ax_boot.set_title(f'Bootstrap Distribution\\n(B={B})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "ax_boot.legend(fontsize=9)\n",
    "ax_boot.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Bootstrap CI for Correlation Coefficient üî¨', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Bootstrap for Correlation:\")\n",
    "print(\"   - No complicated formulas needed\")\n",
    "print(\"   - Accounts for non-normality automatically\")\n",
    "print(\"   - CI shows uncertainty in correlation estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåæ Bootstrap for difference in means (two treatments)\n",
    "\n",
    "# Two fertilizer treatments\n",
    "np.random.seed(42)\n",
    "treatment_A = np.random.normal(5.2, 0.7, 30)  # Standard fertilizer\n",
    "treatment_B = np.random.normal(5.6, 0.8, 35)  # New fertilizer\n",
    "\n",
    "# Observed difference\n",
    "observed_diff = treatment_B.mean() - treatment_A.mean()\n",
    "\n",
    "# Bootstrap for difference\n",
    "B = 2000\n",
    "bootstrap_diffs = []\n",
    "\n",
    "for _ in range(B):\n",
    "    # Resample each group independently\n",
    "    boot_A = np.random.choice(treatment_A, size=len(treatment_A), replace=True)\n",
    "    boot_B = np.random.choice(treatment_B, size=len(treatment_B), replace=True)\n",
    "    \n",
    "    # Calculate difference\n",
    "    diff = boot_B.mean() - boot_A.mean()\n",
    "    bootstrap_diffs.append(diff)\n",
    "\n",
    "bootstrap_diffs = np.array(bootstrap_diffs)\n",
    "\n",
    "# Calculate CI\n",
    "diff_ci_lower = np.percentile(bootstrap_diffs, 2.5)\n",
    "diff_ci_upper = np.percentile(bootstrap_diffs, 97.5)\n",
    "diff_se = bootstrap_diffs.std()\n",
    "\n",
    "print(\"üåæ Bootstrap CI for Difference in Means (Two Treatments):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Treatment A (standard): n = {len(treatment_A)}, mean = {treatment_A.mean():.3f}\")\n",
    "print(f\"Treatment B (new):      n = {len(treatment_B)}, mean = {treatment_B.mean():.3f}\")\n",
    "print(f\"\\nRESULTS:\")\n",
    "print(f\"  Observed difference (B - A): {observed_diff:.4f} tons/hectare\")\n",
    "print(f\"  Bootstrap SE: {diff_se:.4f}\")\n",
    "print(f\"  95% Bootstrap CI: [{diff_ci_lower:.4f}, {diff_ci_upper:.4f}]\")\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "if diff_ci_lower > 0:\n",
    "    print(f\"   Treatment B is significantly better than A (CI doesn't include 0)\")\n",
    "    print(f\"   Improvement: {observed_diff:.2f} tons/hectare\")\n",
    "else:\n",
    "    print(f\"   Cannot conclude B is better (CI includes 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 6: Difference distribution\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.hist(bootstrap_diffs, bins=40, alpha=0.7, color='steelblue', \n",
    "         edgecolor='black', density=True, label='Bootstrap Distribution')\n",
    "\n",
    "# Shade CI\n",
    "x_fill = bootstrap_diffs[(bootstrap_diffs >= diff_ci_lower) & \n",
    "                         (bootstrap_diffs <= diff_ci_upper)]\n",
    "plt.hist(x_fill, bins=40, alpha=0.5, color='green', \n",
    "         edgecolor='none', density=True, label='95% CI')\n",
    "\n",
    "# Mark observed difference\n",
    "plt.axvline(observed_diff, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Observed Diff = {observed_diff:.3f}')\n",
    "\n",
    "# Mark zero (no difference)\n",
    "plt.axvline(0, color='black', linestyle='-', linewidth=2, alpha=0.5,\n",
    "            label='No difference (0)')\n",
    "\n",
    "# Mark CI bounds\n",
    "plt.axvline(diff_ci_lower, color='green', linestyle=':', linewidth=2)\n",
    "plt.axvline(diff_ci_upper, color='green', linestyle=':', linewidth=2)\n",
    "\n",
    "plt.xlabel('Difference in Mean Yield (B - A, tons/hectare)', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.title('Bootstrap Distribution of Treatment Difference üåæ', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add text\n",
    "if diff_ci_lower > 0:\n",
    "    color = 'lightgreen'\n",
    "    msg = f'CI excludes 0\\nTreatment B\\nis better!'\n",
    "else:\n",
    "    color = 'lightyellow'\n",
    "    msg = f'CI includes 0\\nNo clear\\nwinner'\n",
    "\n",
    "textstr = f'95% CI:\\n[{diff_ci_lower:.3f}, {diff_ci_upper:.3f}]\\n\\n{msg}'\n",
    "props = dict(boxstyle='round', facecolor=color, alpha=0.8)\n",
    "plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,\n",
    "         verticalalignment='top', bbox=props)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Using Bootstrap for Treatment Comparison:\")\n",
    "print(\"   - If CI excludes 0: Significant difference\")\n",
    "print(\"   - If CI includes 0: No significant difference\")\n",
    "print(\"   - More flexible than t-test (no normality assumption)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. When to Use Bootstrap ‚öñÔ∏è\n",
    "\n",
    "### Bootstrap is Great When:\n",
    "\n",
    "‚úÖ **Complex statistics**: No formula for SE (correlation, percentiles, ratios)\n",
    "\n",
    "‚úÖ **Non-normal data**: Don't want to assume normality\n",
    "\n",
    "‚úÖ **Small-to-moderate samples**: n ‚â• 20-30 typically works well\n",
    "\n",
    "‚úÖ **Flexible inference**: Want to avoid restrictive assumptions\n",
    "\n",
    "‚úÖ **Quick implementation**: Same algorithm for any statistic\n",
    "\n",
    "### Use Caution When:\n",
    "\n",
    "‚ö†Ô∏è **Very small samples**: n < 20 may not provide stable estimates\n",
    "\n",
    "‚ö†Ô∏è **Estimating extremes**: Max, min, extreme percentiles (sample may not contain extreme values)\n",
    "\n",
    "‚ö†Ô∏è **Heavy dependence**: Time series with strong autocorrelation (need block bootstrap)\n",
    "\n",
    "‚ö†Ô∏è **Computational cost**: Very large datasets + many iterations can be slow\n",
    "\n",
    "### Bootstrap vs Classical:\n",
    "\n",
    "| Aspect | Classical | Bootstrap |\n",
    "|--------|-----------|----------|\n",
    "| **Assumptions** | Normality, known distribution | Minimal |\n",
    "| **Formulas** | Need specific formula for each | Same algorithm for all |\n",
    "| **Flexibility** | Limited | High |\n",
    "| **Computation** | Fast (formula) | Slower (resampling) |\n",
    "| **Small samples** | May be better | May be unstable |\n",
    "| **Complex statistics** | Often difficult/impossible | Easy |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Machine Learning Connection ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "### Bootstrap ‚Üí Bagging ‚Üí Random Forests\n",
    "\n",
    "**This is where statistical inference meets modern ML!**\n",
    "\n",
    "#### 1. Bootstrap Aggregating (Bagging) üéí\n",
    "\n",
    "**Idea**: Reduce variance by averaging predictions from multiple models\n",
    "\n",
    "**Algorithm**:\n",
    "1. FOR b = 1 to B:\n",
    "   - Generate bootstrap sample from training data\n",
    "   - Train model on bootstrap sample ‚Üí model_b\n",
    "2. Final prediction = AVERAGE of all model predictions\n",
    "\n",
    "**Why it works**: Central Limit Theorem!\n",
    "- Individual models have high variance\n",
    "- Averaging reduces variance (CLT)\n",
    "- Bootstrap provides diversity\n",
    "\n",
    "#### 2. Random Forests üå≤\n",
    "\n",
    "**Random Forest = Bagging + Decision Trees + Random Feature Selection**\n",
    "\n",
    "1. Bootstrap samples (like bagging)\n",
    "2. Train decision tree on each sample\n",
    "3. At each split: consider only random subset of features\n",
    "4. Average predictions from all trees\n",
    "\n",
    "**Result**: One of the most powerful ML algorithms!\n",
    "\n",
    "#### 3. Why Ensemble Methods Work:\n",
    "\n",
    "- **Unstable models** (high variance): Decision trees, neural nets\n",
    "- **Bootstrap creates diversity**: Different training sets\n",
    "- **Averaging reduces variance**: CLT in action\n",
    "- **Better generalization**: More robust predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ Bagging demonstration\n",
    "\n",
    "# Generate synthetic agricultural data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X, y = make_regression(n_samples=n_samples, n_features=1, noise=15, random_state=42)\n",
    "X = X.ravel()\n",
    "\n",
    "# Train single decision tree (unstable, high variance)\n",
    "single_tree = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "single_tree.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "# Bagging: Train multiple trees on bootstrap samples\n",
    "n_trees = 100\n",
    "bagged_predictions = []\n",
    "individual_trees = []\n",
    "\n",
    "for i in range(n_trees):\n",
    "    # Bootstrap sample\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    X_boot = X[indices]\n",
    "    y_boot = y[indices]\n",
    "    \n",
    "    # Train tree\n",
    "    tree = DecisionTreeRegressor(max_depth=5, random_state=i)\n",
    "    tree.fit(X_boot.reshape(-1, 1), y_boot)\n",
    "    individual_trees.append(tree)\n",
    "    \n",
    "    # Predictions for visualization\n",
    "    X_test = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "    bagged_predictions.append(tree.predict(X_test))\n",
    "\n",
    "bagged_predictions = np.array(bagged_predictions)\n",
    "ensemble_prediction = bagged_predictions.mean(axis=0)\n",
    "\n",
    "print(\"ü§ñ Bootstrap Aggregating (Bagging) Demonstration:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training data: n = {n_samples} agricultural observations\")\n",
    "print(f\"Model: Decision Tree (max_depth=5)\")\n",
    "print(f\"Number of bagged models: {n_trees}\")\n",
    "print(f\"\\nAPPROACH:\")\n",
    "print(f\"  1. Generate {n_trees} bootstrap samples from training data\")\n",
    "print(f\"  2. Train decision tree on each bootstrap sample\")\n",
    "print(f\"  3. Final prediction = AVERAGE of all {n_trees} tree predictions\")\n",
    "print(f\"\\nüí° This is exactly how Random Forests work!\")\n",
    "print(f\"   (Plus random feature selection at each split)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Visualization 7: Bagging reduces variance\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "X_test = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
    "\n",
    "# Left: Individual bootstrap predictions\n",
    "axes[0].scatter(X, y, alpha=0.4, s=50, color='gray', label='Training Data')\n",
    "\n",
    "# Plot 20 individual tree predictions\n",
    "for i in range(20):\n",
    "    axes[0].plot(X_test, bagged_predictions[i], 'b-', alpha=0.15, linewidth=1)\n",
    "\n",
    "# Highlight one\n",
    "axes[0].plot(X_test, bagged_predictions[0], 'b-', alpha=0.5, linewidth=2,\n",
    "             label='Individual Tree Predictions')\n",
    "\n",
    "axes[0].set_xlabel('Feature', fontsize=11)\n",
    "axes[0].set_ylabel('Target', fontsize=11)\n",
    "axes[0].set_title('Individual Bootstrap Trees\\n(High Variance)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Ensemble prediction\n",
    "axes[1].scatter(X, y, alpha=0.4, s=50, color='gray', label='Training Data')\n",
    "\n",
    "# Show a few individual predictions for reference\n",
    "for i in range(10):\n",
    "    axes[1].plot(X_test, bagged_predictions[i], 'b-', alpha=0.1, linewidth=1)\n",
    "\n",
    "# Ensemble prediction (averaged)\n",
    "axes[1].plot(X_test, ensemble_prediction, 'r-', linewidth=3, \n",
    "             label=f'Bagged Ensemble (avg of {n_trees} trees)', zorder=5)\n",
    "\n",
    "axes[1].set_xlabel('Feature', fontsize=11)\n",
    "axes[1].set_ylabel('Target', fontsize=11)\n",
    "axes[1].set_title(f'Bagged Ensemble (Average of {n_trees} Trees)\\n(Low Variance!)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Bootstrap Aggregating: Variance Reduction Through Averaging üéØ', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key ML Insights:\")\n",
    "print(\"   LEFT: Individual trees are 'wiggly' (high variance)\")\n",
    "print(\"   RIGHT: Averaged prediction is smooth (low variance)\")\n",
    "print(\"\\nüéØ This is why Random Forests work so well:\")\n",
    "print(\"   1. Bootstrap creates diverse training sets\")\n",
    "print(\"   2. Each tree learns different patterns\")\n",
    "print(\"   3. Averaging reduces variance (CLT!)\")\n",
    "print(\"   4. Final model generalizes better\")\n",
    "print(\"\\n‚ú® Bootstrap ‚Üí Bagging ‚Üí Random Forests!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways üéØ\n",
    "\n",
    "### Bootstrap Methods:\n",
    "\n",
    "1. ‚úÖ **Core Idea** ‚≠ê:\n",
    "   - Treat sample as mini-population\n",
    "   - Resample WITH replacement (B times)\n",
    "   - Use bootstrap distribution for inference\n",
    "\n",
    "2. ‚úÖ **Bootstrap SE**:\n",
    "   - SE = Standard deviation of bootstrap statistics\n",
    "   - Works for ANY statistic (no formula needed!)\n",
    "   - Mean, median, correlation, custom metrics‚Äîall the same algorithm\n",
    "\n",
    "3. ‚úÖ **Bootstrap CI** (Percentile Method):\n",
    "   - 95% CI = [2.5th percentile, 97.5th percentile]\n",
    "   - No distributional assumptions\n",
    "   - Handles skewed distributions naturally\n",
    "\n",
    "4. ‚úÖ **Advantages**:\n",
    "   - Minimal assumptions\n",
    "   - Works for complex statistics\n",
    "   - Same algorithm for everything\n",
    "   - Flexible and modern\n",
    "\n",
    "5. ‚úÖ **ML Connection** ‚≠ê‚≠ê‚≠ê:\n",
    "   - **Bootstrap ‚Üí Bagging**: Train on bootstrap samples, average predictions\n",
    "   - **Random Forests** = Bootstrap + Trees + Random features\n",
    "   - **Variance reduction**: Averaging works (CLT!)\n",
    "   - **Ensemble methods**: Understanding bootstrap ‚Üí understanding why they work\n",
    "\n",
    "### The Bootstrap-to-ML Pipeline:\n",
    "\n",
    "```\n",
    "Bootstrap Resampling\n",
    "    ‚Üì\n",
    "Multiple diverse training sets\n",
    "    ‚Üì\n",
    "Train model on each (Bootstrap Aggregating = Bagging)\n",
    "    ‚Üì\n",
    "Average predictions (CLT reduces variance)\n",
    "    ‚Üì\n",
    "Better generalization!\n",
    "    ‚Üì\n",
    "Random Forests, Gradient Boosting, Ensemble Methods\n",
    "```\n",
    "\n",
    "### Critical Formulas:\n",
    "\n",
    "$$\n",
    "\\boxed{SE_{bootstrap} = SD(\\{\\theta^*_1, \\theta^*_2, ..., \\theta^*_B\\})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\text{95% CI} = [P_{2.5}, P_{97.5}] \\text{ (percentile method)}}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-Up: The Power of Bootstrap üéí\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "Bootstrap is a **revolutionary approach to statistical inference**:\n",
    "\n",
    "1. **Flexibility**: Works for any statistic, any distribution\n",
    "2. **Simplicity**: Same algorithm every time\n",
    "3. **Power**: Handles situations where classical methods fail\n",
    "4. **Modern**: Foundation of many ML techniques\n",
    "\n",
    "### From Statistics to Machine Learning:\n",
    "\n",
    "Understanding bootstrap helps you understand:\n",
    "- **Why Random Forests work** (bootstrap + trees)\n",
    "- **Why ensemble methods are powerful** (averaging reduces variance)\n",
    "- **How to evaluate models robustly** (bootstrap validation)\n",
    "- **How to quantify uncertainty in ML** (bootstrap CIs for performance)\n",
    "\n",
    "### Next Module: Hypothesis Testing üî¨\n",
    "\n",
    "In the next module (06-Hypothesis-Testing), we'll learn:\n",
    "- Making decisions from data\n",
    "- p-values and significance testing\n",
    "- Type I and Type II errors\n",
    "- ML connection: A/B testing, model comparison\n",
    "\n",
    "But that's for the next module!\n",
    "\n",
    "---\n",
    "\n",
    "### You've Completed Statistical Inference Phase 1! üéâ\n",
    "\n",
    "**You now understand**:\n",
    "1. ‚úÖ Sampling and sampling distributions\n",
    "2. ‚úÖ Central Limit Theorem (why inference works)\n",
    "3. ‚úÖ Point estimation and MLE (training is estimation!)\n",
    "4. ‚úÖ Confidence intervals (quantifying uncertainty)\n",
    "5. ‚úÖ Bootstrap methods (modern flexible inference)\n",
    "\n",
    "**And you see how it all connects to ML**:\n",
    "- Cross-validation = Sampling\n",
    "- Ensemble averaging = CLT\n",
    "- Training = MLE\n",
    "- Model performance CIs = Confidence intervals\n",
    "- **Random Forests = Bootstrap + Trees**\n",
    "\n",
    "**Statistical inference IS the foundation of machine learning!** üéØ‚ú®üåæ\n",
    "\n",
    "---\n",
    "\n",
    "**Excellent work completing Phase 1 (Fundamentals)!**\n",
    "\n",
    "**Next**: Phase 2 (From Scratch), Phase 3 (With SciPy), Phase 4 (Agricultural Applications)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
